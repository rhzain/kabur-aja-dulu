{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19494fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhzain\\AppData\\Local\\Temp\\ipykernel_33724\\1846252205.py:8: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import demoji\n",
    "\n",
    "# Download emoji data (run once)\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2385c25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total komentar: 778\n",
      "\n",
      "Contoh data awal:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video_ID</th>\n",
       "      <th>Teks_Komentar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>Sempat mikir mau pindah ke negara sebelah, nge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>Kalo kabur mau kemana ke Singapur ,emang di Si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>Klo sudah gelap susah terangnya lebih baik bubar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>Pengen kabur tapi gak punya uangüò≠</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>Kalau mau pindah ke luar negeri ya silahkan sa...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Video_ID                                      Teks_Komentar\n",
       "0  MIo4tGN11j0  Sempat mikir mau pindah ke negara sebelah, nge...\n",
       "1  MIo4tGN11j0  Kalo kabur mau kemana ke Singapur ,emang di Si...\n",
       "2  MIo4tGN11j0   Klo sudah gelap susah terangnya lebih baik bubar\n",
       "3  MIo4tGN11j0                  Pengen kabur tapi gak punya uangüò≠\n",
       "4  MIo4tGN11j0  Kalau mau pindah ke luar negeri ya silahkan sa..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_filtered.csv')\n",
    "print(f\"Total komentar: {len(df)}\")\n",
    "print(\"\\nContoh data awal:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e655cd7",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Teks\n",
    "\n",
    "Melakukan preprocessing data dengan 5 tahap terpisah yang modular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7e080c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INISIALISASI PREPROCESSING TOOLS\n",
      "============================================================\n",
      "‚úÖ Sastrawi Stemmer berhasil dimuat\n",
      "‚ö†Ô∏è  WARNING: Library indoNLP tidak ditemukan.\n",
      "   Silakan instal dengan: pip install indonlp\n",
      "   Preprocessing akan tetap berjalan tanpa indoNLP.\n",
      "‚úÖ KEYWORD dimuat: 37 frasa ‚Üí 43 kata unik\n",
      "‚úÖ Stopwords berhasil dimuat:\n",
      "   - Indonesia       : 757 kata\n",
      "   - Inggris         : 198 kata\n",
      "   - Gaul + Rojak    : 51 kata\n",
      "   - TOTAL           : 985 kata (setelah exclude negasi & keyword)\n",
      "   - Negasi (PROTECT): {'gak', 'belum', 'ndak', 'nggak', 'jangan', 'tanpa', 'engga', 'ga', 'bukan', 'tidak', 'enggak'}\n",
      "   - Keyword (PROTECT): 43 kata\n",
      "\n",
      "============================================================\n",
      "‚úÖ Semua fungsi preprocessing pipeline sudah siap!\n",
      "\n",
      "üìã URUTAN PIPELINE:\n",
      "   Tahap 1: Filter Konteks & Noise\n",
      "   Tahap 2: Cleaning (URL, HTML, Hapus Emoji, Slang, Elongasi, Tanda Baca)\n",
      "   Tahap 3: Tokenisasi + Negation Tagging\n",
      "   Tahap 4: Stemming (skip keyword & tag)\n",
      "   Tahap 5: Stopword Removal (preserve negasi & keyword)\n",
      "   Tahap 6: Gabung Kembali\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import demoji\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- 1. Inisialisasi Alat (Sekali saja) ---\n",
    "print(\"=\"*60)\n",
    "print(\"INISIALISASI PREPROCESSING TOOLS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Impor Stemmer dari Sastrawi\n",
    "try:\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    print(\"‚úÖ Sastrawi Stemmer berhasil dimuat\")\n",
    "    factory = StemmerFactory()\n",
    "    stemmer_sastrawi = factory.create_stemmer()\n",
    "except ImportError:\n",
    "    print(\"‚ùå ERROR: Library Sastrawi tidak ditemukan.\")\n",
    "    print(\"   Silakan instal dengan: pip install Sastrawi\")\n",
    "    raise\n",
    "\n",
    "# Import indoNLP untuk preprocessing slang dan elongation\n",
    "try:\n",
    "    from indonlp.preprocessing import replace_slang, replace_word_elongation\n",
    "    print(\"‚úÖ indoNLP preprocessing berhasil dimuat\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Library indoNLP tidak ditemukan.\")\n",
    "    print(\"   Silakan instal dengan: pip install indonlp\")\n",
    "    print(\"   Preprocessing akan tetap berjalan tanpa indoNLP.\")\n",
    "    replace_slang = lambda x: x\n",
    "    replace_word_elongation = lambda x: x\n",
    "\n",
    "# --- 2. Buat Daftar KEYWORD untuk Filter (dari Cell 4) ---\n",
    "KEYWORD = [\n",
    "    'kabur', 'pindah negara', 'leave indo', 'pajak', 'gaji', 'umr', \n",
    "    'biaya hidup', 'korupsi', 'sandwich', 'luar negeri', \n",
    "    'paspor', 'warga negara', 'singapur', 'singapore', 'australia', 'aussie', 'jepang', 'eropa', 'wni',\n",
    "    'capek', 'lelah', 'pemerintah', 'mending', 'percuma', 'beban', \n",
    "    'suram', 'males', 'ga jelas', 'ga ada harapan', 'nyicil', 'susah',\n",
    "    'setuju', 'sulit', 'stres', 'politik', 'birokrasi', 'konoha'\n",
    "]\n",
    "\n",
    "# Buat set untuk pencarian cepat saat stemming\n",
    "KEYWORD_SET = set()\n",
    "for keyword in KEYWORD:\n",
    "    # Pecah multi-word keywords jadi single word\n",
    "    KEYWORD_SET.update(keyword.lower().split())\n",
    "\n",
    "print(f\"‚úÖ KEYWORD dimuat: {len(KEYWORD)} frasa ‚Üí {len(KEYWORD_SET)} kata unik\")\n",
    "\n",
    "# --- 3. Buat Daftar Stopwords Gabungan ---\n",
    "stop_words_indo = set(stopwords.words('indonesian'))\n",
    "stop_words_eng = set(stopwords.words('english'))\n",
    "\n",
    "# Stopwords gaul dan kata rojak (bahasa Inggris umum)\n",
    "custom_stopwords_gaul = {\n",
    "    'yg', 'dg', 'rt', 'dgn', 'ny', 'd', 'klo', 'kalo', 'amp', 'biar', 'bkn', 'na', \n",
    "    'nya', 'nih', 'sih', 'si', 'tau', 'tuh', 'utk', 'ya', 'gaes',\n",
    "    'bang', 'bro', 'sob', 'gw', 'gua', 'lu', 'lo', 'wkwk', 'haha', 'wkwkwk', \n",
    "    'amin', 'amiin', 'aamiin', 'yuk', 'dong', 'deh', 'kok',\n",
    "    # Kata rojak (bahasa Inggris umum yang tidak bisa di-stem)\n",
    "    'government', 'tax', 'salary', 'system', 'netizen', 'the', 'and', 'or', 'in', 'of', 'to', 'is', 'for'\n",
    "}\n",
    "\n",
    "# --- 4. Daftar Kata NEGASI yang TIDAK BOLEH dihapus ---\n",
    "NEGASI_WORDS = {\n",
    "    'tidak', 'bukan', 'jangan', 'ga', 'gak', 'enggak', 'nggak', 'ndak', 'engga', 'belum', 'tanpa'\n",
    "}\n",
    "\n",
    "# Gabungkan semua stopwords, KECUALI kata negasi dan keyword penting\n",
    "stop_words_final = (stop_words_indo.union(stop_words_eng, custom_stopwords_gaul)) - NEGASI_WORDS - KEYWORD_SET\n",
    "\n",
    "print(\"‚úÖ Stopwords berhasil dimuat:\")\n",
    "print(f\"   - Indonesia       : {len(stop_words_indo)} kata\")\n",
    "print(f\"   - Inggris         : {len(stop_words_eng)} kata\")\n",
    "print(f\"   - Gaul + Rojak    : {len(custom_stopwords_gaul)} kata\")\n",
    "print(f\"   - TOTAL           : {len(stop_words_final)} kata (setelah exclude negasi & keyword)\")\n",
    "print(f\"   - Negasi (PROTECT): {NEGASI_WORDS}\")\n",
    "print(f\"   - Keyword (PROTECT): {len(KEYWORD_SET)} kata\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "\n",
    "# --- 5. Definisi Fungsi Preprocessing Pipeline ---\n",
    "\n",
    "def tahap1_cleaning(teks):\n",
    "    \"\"\"\n",
    "    Tahap 2: Membersihkan teks mentah dari URL, HTML, emoji,\n",
    "    normalisasi slang, elongasi, tanda baca, dan angka.\n",
    "    \n",
    "    DENGAN indoNLP untuk slang dan elongation!\n",
    "    \"\"\"\n",
    "    if pd.isna(teks):\n",
    "        return \"\"\n",
    "    \n",
    "    teks = str(teks)\n",
    "    \n",
    "    # 1. Hapus URL\n",
    "    teks = re.sub(r'http\\S+|www\\.\\S+', '', teks)\n",
    "    \n",
    "    # 2. Hapus HTML tags\n",
    "    teks = re.sub(r'<.*?>', '', teks)\n",
    "    \n",
    "    # 3. Hapus SEMUA emoji (tanpa konversi ke tag)\n",
    "    teks = demoji.replace(teks, '')\n",
    "    \n",
    "    # 4. Normalisasi Unicode (font aneh)\n",
    "    teks = unicodedata.normalize('NFKD', teks)\n",
    "    \n",
    "    # 5. Lowercase\n",
    "    teks = teks.lower()\n",
    "    \n",
    "    # 6. Ganti slang menggunakan indoNLP\n",
    "    teks = replace_slang(teks)\n",
    "    \n",
    "    # 7. Ganti elongasi (kata berulang) menggunakan indoNLP\n",
    "    teks = replace_word_elongation(teks)\n",
    "    \n",
    "    # 8. Hapus tanda baca dan angka (sisakan huruf dan spasi)\n",
    "    teks = re.sub(r'[^a-z\\s]', ' ', teks)\n",
    "    \n",
    "    # 9. Hapus spasi berlebih\n",
    "    teks = re.sub(r'\\s+', ' ', teks).strip()\n",
    "    \n",
    "    return teks\n",
    "\n",
    "\n",
    "def tahap2_tokenisasi(teks):\n",
    "    \"\"\"\n",
    "    Tahap 3a: Tokenisasi - Memecah teks menjadi list kata-kata.\n",
    "    \"\"\"\n",
    "    if not teks or pd.isna(teks):\n",
    "        return []\n",
    "    \n",
    "    # Pecah berdasarkan spasi\n",
    "    tokens = teks.split()\n",
    "    \n",
    "    # Filter kata yang terlalu pendek (< 3 karakter), KECUALI kata negasi pendek\n",
    "    tokens = [kata for kata in tokens if len(kata) >= 3 or kata in {'ga', 'gak'}]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tahap2_5_negation_tagging(tokens):\n",
    "    \"\"\"\n",
    "    Tahap 3b: Negation Tagging - Menggabungkan kata negasi dengan kata berikutnya.\n",
    "    \n",
    "    Implementasi negation tagging untuk analisis sentimen yang lebih baik.\n",
    "    Contoh: ['gaji', 'ga', 'naik'] -> ['gaji', 'TIDAK_naik']\n",
    "    \"\"\"\n",
    "    if not tokens or len(tokens) == 0:\n",
    "        return []\n",
    "    \n",
    "    tokens_tagged = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(tokens):\n",
    "        current_token = tokens[i]\n",
    "        \n",
    "        # Jika kata ini adalah negasi DAN ada kata setelahnya\n",
    "        if current_token in NEGASI_WORDS and i + 1 < len(tokens):\n",
    "            next_token = tokens[i + 1]\n",
    "            # Gabungkan negasi dengan kata berikutnya\n",
    "            tagged_token = f\"TIDAK_{next_token}\"\n",
    "            tokens_tagged.append(tagged_token)\n",
    "            # Skip kata berikutnya karena sudah digabung\n",
    "            i += 2\n",
    "        else:\n",
    "            # Kata biasa, tambahkan seperti biasa\n",
    "            tokens_tagged.append(current_token)\n",
    "            i += 1\n",
    "    \n",
    "    return tokens_tagged\n",
    "\n",
    "\n",
    "def tahap4_stemming(tokens):\n",
    "    \"\"\"\n",
    "    Tahap 4: Stemming - Mengubah kata ke bentuk dasar.\n",
    "    \n",
    "    PENTING: Skip stemming untuk:\n",
    "    - Kata yang ada di KEYWORD_SET (kata kunci penting topik)\n",
    "    - Tag TIDAK_ (negation tagging)\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    tokens_stemmed = []\n",
    "    for kata in tokens:\n",
    "        # Jika kata adalah keyword penting, JANGAN di-stem\n",
    "        if kata in KEYWORD_SET:\n",
    "            tokens_stemmed.append(kata)\n",
    "        # Jika kata mengandung tag TIDAK_, stem hanya bagian kata dasarnya\n",
    "        elif kata.startswith('TIDAK_'):\n",
    "            # Ambil kata setelah TIDAK_\n",
    "            kata_dasar = kata.replace('TIDAK_', '')\n",
    "            # Cek apakah kata dasar adalah keyword\n",
    "            if kata_dasar in KEYWORD_SET:\n",
    "                # Jangan stem, pertahankan dengan tag TIDAK_\n",
    "                tokens_stemmed.append(kata)\n",
    "            else:\n",
    "                # Stem kata dasarnya\n",
    "                kata_stemmed = stemmer_sastrawi.stem(kata_dasar)\n",
    "                # Gabung kembali dengan tag TIDAK_\n",
    "                tokens_stemmed.append(f'TIDAK_{kata_stemmed}')\n",
    "        else:\n",
    "            # Kata biasa, stem seperti biasa\n",
    "            tokens_stemmed.append(stemmer_sastrawi.stem(kata))\n",
    "    \n",
    "    return tokens_stemmed\n",
    "\n",
    "\n",
    "def tahap3_stopword_removal(tokens):\n",
    "    \"\"\"\n",
    "    Tahap 5: Hapus Stopwords dari list tokens.\n",
    "    \n",
    "    Dilakukan SETELAH stemming.\n",
    "    JANGAN hapus: tag TIDAK_ dan keyword penting.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    tokens_bersih = []\n",
    "    for kata in tokens:\n",
    "        # Jangan hapus kata yang mengandung tag TIDAK_\n",
    "        if kata.startswith('TIDAK_'):\n",
    "            tokens_bersih.append(kata)\n",
    "        # Jangan hapus keyword penting\n",
    "        elif kata in KEYWORD_SET:\n",
    "            tokens_bersih.append(kata)\n",
    "        # Hapus stopwords biasa\n",
    "        elif kata not in stop_words_final:\n",
    "            tokens_bersih.append(kata)\n",
    "    \n",
    "    return tokens_bersih\n",
    "\n",
    "\n",
    "def tahap5_gabung_kembali(tokens):\n",
    "    \"\"\"\n",
    "    Tahap 6: Gabungkan tokens menjadi teks final.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return \"\"\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"‚úÖ Semua fungsi preprocessing pipeline sudah siap!\")\n",
    "print(\"\\nüìã URUTAN PIPELINE:\")\n",
    "print(\"   Tahap 1: Filter Konteks & Noise\")\n",
    "print(\"   Tahap 2: Cleaning (URL, HTML, Hapus Emoji, Slang, Elongasi, Tanda Baca)\")\n",
    "print(\"   Tahap 3: Tokenisasi + Negation Tagging\")\n",
    "print(\"   Tahap 4: Stemming (skip keyword & tag)\")\n",
    "print(\"   Tahap 5: Stopword Removal (preserve negasi & keyword)\")\n",
    "print(\"   Tahap 6: Gabung Kembali\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beeb1d1",
   "metadata": {},
   "source": [
    "### Tahap 1: Cleaning\n",
    "\n",
    "Membersihkan teks dari URL, HTML, emoji, tanda baca, dan angka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66bba9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 2/6: CLEANING\n",
      "============================================================\n",
      "Membersihkan URL, HTML, emoji‚Üítag, slang, elongasi, tanda baca...\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil cleaning:\n",
      "ASLI  : Sempat mikir mau pindah ke negara sebelah, ngeapply citizenship. Tapi yah, aku c...\n",
      "CLEAN : sempat mikir mau pindah ke negara sebelah ngeapply citizenship tapi yah aku cint...\n",
      "============================================================\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil cleaning:\n",
      "ASLI  : Sempat mikir mau pindah ke negara sebelah, ngeapply citizenship. Tapi yah, aku c...\n",
      "CLEAN : sempat mikir mau pindah ke negara sebelah ngeapply citizenship tapi yah aku cint...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 2/6: CLEANING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Membersihkan URL, HTML, emoji‚Üítag, slang, elongasi, tanda baca...\")\n",
    "\n",
    "df['teks_tahap1'] = df['Teks_Komentar'].apply(tahap1_cleaning)\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"\\nContoh hasil cleaning:\")\n",
    "print(f\"ASLI  : {df['Teks_Komentar'].iloc[0][:80]}...\")\n",
    "print(f\"CLEAN : {df['teks_tahap1'].iloc[0][:80]}...\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4906f4a",
   "metadata": {},
   "source": [
    "### Tahap 3: Tokenisasi & Negation Tagging\n",
    "\n",
    "Memecah teks menjadi kata-kata dan menggabungkan kata negasi dengan kata berikutnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63d5e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 3/6: TOKENISASI & NEGATION TAGGING\n",
      "============================================================\n",
      "Memecah teks menjadi kata-kata...\n",
      "‚úÖ Tokenisasi selesai!\n",
      "\n",
      "Contoh hasil tokenisasi:\n",
      "TEKS   : sempat mikir mau pindah ke negara sebelah ngeapply citizensh...\n",
      "TOKENS : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'sebelah', 'ngeapply', 'citizenship']...\n",
      "JUMLAH : 21 kata\n",
      "\n",
      "Menggabungkan kata negasi dengan kata berikutnya...\n",
      "‚úÖ Negation tagging selesai!\n",
      "\n",
      "Contoh hasil negation tagging:\n",
      "SEBELUM : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'sebelah', 'ngeapply', 'citizenship']...\n",
      "SESUDAH : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'sebelah', 'ngeapply', 'citizenship']...\n",
      "\n",
      "üí° Contoh: 'gaji ga naik' ‚Üí 'gaji TIDAK_naik'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 3/6: TOKENISASI & NEGATION TAGGING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Memecah teks menjadi kata-kata...\")\n",
    "\n",
    "# Tahap 3a: Tokenisasi\n",
    "df['tokens_tahap2'] = df['teks_tahap1'].apply(tahap2_tokenisasi)\n",
    "\n",
    "print(\"‚úÖ Tokenisasi selesai!\")\n",
    "print(f\"\\nContoh hasil tokenisasi:\")\n",
    "tokens_contoh = df['tokens_tahap2'].iloc[0]\n",
    "print(f\"TEKS   : {df['teks_tahap1'].iloc[0][:60]}...\")\n",
    "print(f\"TOKENS : {tokens_contoh[:8]}...\")\n",
    "print(f\"JUMLAH : {len(tokens_contoh)} kata\")\n",
    "\n",
    "# Tahap 3b: Negation Tagging\n",
    "print(\"\\nMenggabungkan kata negasi dengan kata berikutnya...\")\n",
    "df['tokens_tahap2_5'] = df['tokens_tahap2'].apply(tahap2_5_negation_tagging)\n",
    "\n",
    "print(\"‚úÖ Negation tagging selesai!\")\n",
    "print(f\"\\nContoh hasil negation tagging:\")\n",
    "tokens_before = df['tokens_tahap2'].iloc[0]\n",
    "tokens_after = df['tokens_tahap2_5'].iloc[0]\n",
    "print(f\"SEBELUM : {tokens_before[:8]}...\")\n",
    "print(f\"SESUDAH : {tokens_after[:8]}...\")\n",
    "print(f\"\\nüí° Contoh: 'gaji ga naik' ‚Üí 'gaji TIDAK_naik'\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010b376",
   "metadata": {},
   "source": [
    "### Tahap 4: Stemming\n",
    "\n",
    "Mengubah kata ke bentuk dasar (SKIP keyword penting dan tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2233508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 4/6: STEMMING\n",
      "============================================================\n",
      "Mengubah kata ke bentuk dasar...\n",
      "‚ö†Ô∏è  Proses ini lambat, harap sabar...\n",
      "üí° Keyword penting & tag (TIDAK_, _EMOJI_) akan di-skip\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil stemming:\n",
      "SEBELUM : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'sebelah']\n",
      "SESUDAH : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'belah']\n",
      "============================================================\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil stemming:\n",
      "SEBELUM : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'sebelah']\n",
      "SESUDAH : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'belah']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 4/6: STEMMING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Mengubah kata ke bentuk dasar...\")\n",
    "print(\"‚ö†Ô∏è  Proses ini lambat, harap sabar...\")\n",
    "print(\"üí° Keyword penting & tag (TIDAK_, _EMOJI_) akan di-skip\")\n",
    "\n",
    "df['tokens_tahap3'] = df['tokens_tahap2_5'].apply(tahap4_stemming)\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"\\nContoh hasil stemming:\")\n",
    "tokens_before = df['tokens_tahap2_5'].iloc[0]\n",
    "tokens_after = df['tokens_tahap3'].iloc[0]\n",
    "print(f\"SEBELUM : {tokens_before[:6]}\")\n",
    "print(f\"SESUDAH : {tokens_after[:6]}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c84efc2",
   "metadata": {},
   "source": [
    "### Tahap 5: Stopword Removal\n",
    "\n",
    "Menghapus stopwords SETELAH stemming (preserve negasi, emoji, dan keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "553bb734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 5/6: STOPWORD REMOVAL\n",
      "============================================================\n",
      "Menghapus stopwords (preserve: negasi, emoji tag, keyword)...\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil stopword removal:\n",
      "SEBELUM : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'belah', 'ngeapply', 'citizenship']...\n",
      "SESUDAH : ['mikir', 'pindah', 'negara', 'belah', 'ngeapply', 'citizenship', 'yah', 'cinta']...\n",
      "JUMLAH  : 21 kata ‚Üí 14 kata\n",
      "REDUKSI : 7 kata dihapus\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 5/6: STOPWORD REMOVAL\")\n",
    "print(\"=\"*60)\n",
    "print(\"Menghapus stopwords (preserve: negasi, emoji tag, keyword)...\")\n",
    "\n",
    "df['tokens_tahap4'] = df['tokens_tahap3'].apply(tahap3_stopword_removal)\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"\\nContoh hasil stopword removal:\")\n",
    "tokens_before = df['tokens_tahap3'].iloc[0]\n",
    "tokens_after = df['tokens_tahap4'].iloc[0]\n",
    "print(f\"SEBELUM : {tokens_before[:8]}...\")\n",
    "print(f\"SESUDAH : {tokens_after[:8]}...\")\n",
    "print(f\"JUMLAH  : {len(tokens_before)} kata ‚Üí {len(tokens_after)} kata\")\n",
    "print(f\"REDUKSI : {len(tokens_before) - len(tokens_after)} kata dihapus\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c54c741",
   "metadata": {},
   "source": [
    "### Tahap 6: Gabung Kembali & Simpan\n",
    "\n",
    "Menggabungkan tokens menjadi teks final dan menyimpan hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f266a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 6/6: GABUNG KEMBALI & SIMPAN\n",
      "============================================================\n",
      "Menggabungkan tokens menjadi teks final...\n",
      "‚úÖ Selesai!\n",
      "\n",
      "üìä Ringkasan:\n",
      "   Total komentar valid     : 778\n",
      "   Komentar kosong terbuang : 0\n",
      "\n",
      "üíæ Data berhasil disimpan ke 'data_preprocessed.csv'\n",
      "\n",
      "üìã Contoh hasil akhir:\n",
      "\n",
      "   [1] ASLI  : Sempat mikir mau pindah ke negara sebelah, ngeapply citizens...\n",
      "       FINAL : mikir pindah negara belah ngeapply citizenship yah cinta neg...\n",
      "\n",
      "   [2] ASLI  : Kalo kabur mau kemana ke Singapur ,emang di Singapur tinggal...\n",
      "       FINAL : kabur singapur emang singapur tinggal rakyat singapur aja TI...\n",
      "\n",
      "   [3] ASLI  : Klo sudah gelap susah terangnya lebih baik bubar...\n",
      "       FINAL : gelap susah terang bubar...\n",
      "\n",
      "============================================================\n",
      "üéâ PREPROCESSING SELESAI!\n",
      "============================================================\n",
      "\n",
      "üìå PIPELINE YANG DITERAPKAN:\n",
      "   ‚úÖ Tahap 1: Filter Konteks & Noise (Strategi LONGGAR)\n",
      "   ‚úÖ Tahap 2: Cleaning (URL, HTML, Hapus Emoji, Slang, Elongasi)\n",
      "   ‚úÖ Tahap 3: Tokenisasi + Negation Tagging\n",
      "   ‚úÖ Tahap 4: Stemming (skip keyword & tag)\n",
      "   ‚úÖ Tahap 5: Stopword Removal (preserve negasi & keyword)\n",
      "   ‚úÖ Tahap 6: Gabung Kembali\n",
      "\n",
      "üìå FITUR PERBAIKAN:\n",
      "   ‚úÖ Urutan BENAR: Stemming ‚Üí Stopword Removal\n",
      "   ‚úÖ Kata negasi DIPERTAHANKAN & di-tag (ga naik ‚Üí TIDAK_naik)\n",
      "   ‚úÖ Emoji ‚Üí DIHAPUS (tidak digunakan sebagai fitur)\n",
      "   ‚úÖ Kata rojak (government, tax, dll) ‚Üí dihapus\n",
      "   ‚úÖ Keyword penting (pajak, gaji, dll) ‚Üí TIDAK di-stem\n",
      "   ‚úÖ Slang & elongasi ‚Üí dinormalisasi (indoNLP)\n",
      "\n",
      "üìù UNTUK MODELLING:\n",
      "   ‚ö†Ô∏è  Gunakan ngram_range=(1,2) di TF-IDF untuk bigram!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 6/6: GABUNG KEMBALI & SIMPAN\")\n",
    "print(\"=\"*60)\n",
    "print(\"Menggabungkan tokens menjadi teks final...\")\n",
    "\n",
    "df['teks_final'] = df['tokens_tahap4'].apply(tahap5_gabung_kembali)\n",
    "\n",
    "# Buang baris dengan teks kosong setelah preprocessing\n",
    "df_final = df[df['teks_final'].str.strip() != ''].copy()\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"\\nüìä Ringkasan:\")\n",
    "print(f\"   Total komentar valid     : {len(df_final):,}\")\n",
    "print(f\"   Komentar kosong terbuang : {len(df) - len(df_final):,}\")\n",
    "\n",
    "# Simpan hasil akhir\n",
    "output_file = 'data_preprocessed.csv'\n",
    "df_final[['Teks_Komentar', 'teks_final']].to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"\\nüíæ Data berhasil disimpan ke '{output_file}'\")\n",
    "\n",
    "# Tampilkan contoh hasil\n",
    "print(f\"\\nüìã Contoh hasil akhir:\")\n",
    "for i in range(min(3, len(df_final))):\n",
    "    print(f\"\\n   [{i+1}] ASLI  : {df_final['Teks_Komentar'].iloc[i][:60]}...\")\n",
    "    print(f\"       FINAL : {df_final['teks_final'].iloc[i][:60]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ PREPROCESSING SELESAI!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìå PIPELINE YANG DITERAPKAN:\")\n",
    "print(\"   ‚úÖ Tahap 1: Filter Konteks & Noise (Strategi LONGGAR)\")\n",
    "print(\"   ‚úÖ Tahap 2: Cleaning (URL, HTML, Hapus Emoji, Slang, Elongasi)\")\n",
    "print(\"   ‚úÖ Tahap 3: Tokenisasi + Negation Tagging\")\n",
    "print(\"   ‚úÖ Tahap 4: Stemming (skip keyword & tag)\")\n",
    "print(\"   ‚úÖ Tahap 5: Stopword Removal (preserve negasi & keyword)\")\n",
    "print(\"   ‚úÖ Tahap 6: Gabung Kembali\")\n",
    "print(\"\\nüìå FITUR PERBAIKAN:\")\n",
    "print(\"   ‚úÖ Urutan BENAR: Stemming ‚Üí Stopword Removal\")\n",
    "print(\"   ‚úÖ Kata negasi DIPERTAHANKAN & di-tag (ga naik ‚Üí TIDAK_naik)\")\n",
    "print(\"   ‚úÖ Emoji ‚Üí DIHAPUS (tidak digunakan sebagai fitur)\")\n",
    "print(\"   ‚úÖ Kata rojak (government, tax, dll) ‚Üí dihapus\")\n",
    "print(\"   ‚úÖ Keyword penting (pajak, gaji, dll) ‚Üí TIDAK di-stem\")\n",
    "print(\"   ‚úÖ Slang & elongasi ‚Üí dinormalisasi (indoNLP)\")\n",
    "print(\"\\nüìù UNTUK MODELLING:\")\n",
    "print(\"   ‚ö†Ô∏è  Gunakan ngram_range=(1,2) di TF-IDF untuk bigram!\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e1b00",
   "metadata": {},
   "source": [
    "### Tahap 5: Gabung Kembali & Simpan\n",
    "\n",
    "Menggabungkan tokens menjadi teks final dan menyimpan hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7aabee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DETAIL HASIL SETIAP TAHAP PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "CONTOH 1\n",
      "================================================================================\n",
      "\n",
      "0. ASLI:\n",
      "   Kalo kabur mau kemana ke Singapur ,emang di Singapur tinggal dimana rakyat Singapur aja nggak punya ...\n",
      "\n",
      "1. CLEANING (Tahap 2):\n",
      "   kalo kabur mau kemana ke singapur emang di singapur tinggal dimana rakyat singapur aja nggak punya r...\n",
      "\n",
      "2. TOKENISASI (Tahap 3a):\n",
      "   ['kalo', 'kabur', 'mau', 'kemana', 'singapur', 'emang', 'singapur', 'tinggal', 'dimana', 'rakyat']...\n",
      "   (Total: 22 kata)\n",
      "\n",
      "3. NEGATION TAGGING (Tahap 3b):\n",
      "   ['kalo', 'kabur', 'mau', 'kemana', 'singapur', 'emang', 'singapur', 'tinggal', 'dimana', 'rakyat']...\n",
      "   (Total: 21 kata)\n",
      "\n",
      "4. STEMMING (Tahap 4):\n",
      "   ['kalo', 'kabur', 'mau', 'mana', 'singapur', 'emang', 'singapur', 'tinggal', 'mana', 'rakyat']...\n",
      "   (Total: 21 kata)\n",
      "\n",
      "5. STOPWORD REMOVAL (Tahap 5):\n",
      "   ['kabur', 'singapur', 'emang', 'singapur', 'tinggal', 'rakyat', 'singapur', 'aja', 'TIDAK_punya', 'rumah']...\n",
      "   (Total: 14 kata)\n",
      "\n",
      "6. FINAL - GABUNG KEMBALI (Tahap 6):\n",
      "   kabur singapur emang singapur tinggal rakyat singapur aja TIDAK_punya rumah nyicil negara kaya singa...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CONTOH 2\n",
      "================================================================================\n",
      "\n",
      "0. ASLI:\n",
      "   Klo sudah gelap susah terangnya lebih baik bubar...\n",
      "\n",
      "1. CLEANING (Tahap 2):\n",
      "   klo sudah gelap susah terangnya lebih baik bubar...\n",
      "\n",
      "2. TOKENISASI (Tahap 3a):\n",
      "   ['klo', 'sudah', 'gelap', 'susah', 'terangnya', 'lebih', 'baik', 'bubar']...\n",
      "   (Total: 8 kata)\n",
      "\n",
      "3. NEGATION TAGGING (Tahap 3b):\n",
      "   ['klo', 'sudah', 'gelap', 'susah', 'terangnya', 'lebih', 'baik', 'bubar']...\n",
      "   (Total: 8 kata)\n",
      "\n",
      "4. STEMMING (Tahap 4):\n",
      "   ['klo', 'sudah', 'gelap', 'susah', 'terang', 'lebih', 'baik', 'bubar']...\n",
      "   (Total: 8 kata)\n",
      "\n",
      "5. STOPWORD REMOVAL (Tahap 5):\n",
      "   ['gelap', 'susah', 'terang', 'bubar']...\n",
      "   (Total: 4 kata)\n",
      "\n",
      "6. FINAL - GABUNG KEMBALI (Tahap 6):\n",
      "   gelap susah terang bubar...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CONTOH 3\n",
      "================================================================================\n",
      "\n",
      "0. ASLI:\n",
      "   Pengen kabur tapi gak punya uangüò≠...\n",
      "\n",
      "1. CLEANING (Tahap 2):\n",
      "   pengen kabur tapi gak punya uang...\n",
      "\n",
      "2. TOKENISASI (Tahap 3a):\n",
      "   ['pengen', 'kabur', 'tapi', 'gak', 'punya', 'uang']...\n",
      "   (Total: 6 kata)\n",
      "\n",
      "3. NEGATION TAGGING (Tahap 3b):\n",
      "   ['pengen', 'kabur', 'tapi', 'TIDAK_punya', 'uang']...\n",
      "   (Total: 5 kata)\n",
      "\n",
      "4. STEMMING (Tahap 4):\n",
      "   ['ken', 'kabur', 'tapi', 'TIDAK_punya', 'uang']...\n",
      "   (Total: 5 kata)\n",
      "\n",
      "5. STOPWORD REMOVAL (Tahap 5):\n",
      "   ['ken', 'kabur', 'TIDAK_punya', 'uang']...\n",
      "   (Total: 4 kata)\n",
      "\n",
      "6. FINAL - GABUNG KEMBALI (Tahap 6):\n",
      "   ken kabur TIDAK_punya uang...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "*** PERHATIKAN ***\n",
      "   - Kata negasi tetap ada sebagai tag TIDAK_xxx\n",
      "   - Emoji telah DIHAPUS dari data\n",
      "   - Keyword penting (pajak, gaji, dll) tidak di-stem\n",
      "   - Slang sudah dinormalisasi (jika menggunakan indoNLP)\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lihat detail preprocessing untuk 3 contoh pertama\n",
    "print(\"=\"*80)\n",
    "print(\"DETAIL HASIL SETIAP TAHAP PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(3, len(df_final))):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CONTOH {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n0. ASLI:\")\n",
    "    print(f\"   {df_final['Teks_Komentar'].iloc[i][:100]}...\")\n",
    "    \n",
    "    print(f\"\\n1. CLEANING (Tahap 2):\")\n",
    "    print(f\"   {df_final['teks_tahap1'].iloc[i][:100]}...\")\n",
    "    \n",
    "    print(f\"\\n2. TOKENISASI (Tahap 3a):\")\n",
    "    tokens_2 = df_final['tokens_tahap2'].iloc[i]\n",
    "    print(f\"   {tokens_2[:10]}...\")\n",
    "    print(f\"   (Total: {len(tokens_2)} kata)\")\n",
    "    \n",
    "    print(f\"\\n3. NEGATION TAGGING (Tahap 3b):\")\n",
    "    tokens_2_5 = df_final['tokens_tahap2_5'].iloc[i]\n",
    "    print(f\"   {tokens_2_5[:10]}...\")\n",
    "    print(f\"   (Total: {len(tokens_2_5)} kata)\")\n",
    "    \n",
    "    print(f\"\\n4. STEMMING (Tahap 4):\")\n",
    "    tokens_3 = df_final['tokens_tahap3'].iloc[i]\n",
    "    print(f\"   {tokens_3[:10]}...\")\n",
    "    print(f\"   (Total: {len(tokens_3)} kata)\")\n",
    "    \n",
    "    print(f\"\\n5. STOPWORD REMOVAL (Tahap 5):\")\n",
    "    tokens_4 = df_final['tokens_tahap4'].iloc[i]\n",
    "    print(f\"   {tokens_4[:10]}...\")\n",
    "    print(f\"   (Total: {len(tokens_4)} kata)\")\n",
    "    \n",
    "    print(f\"\\n6. FINAL - GABUNG KEMBALI (Tahap 6):\")\n",
    "    print(f\"   {df_final['teks_final'].iloc[i][:100]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\n*** PERHATIKAN ***\")\n",
    "print(\"   - Kata negasi tetap ada sebagai tag TIDAK_xxx\")\n",
    "print(\"   - Emoji telah DIHAPUS dari data\")\n",
    "print(\"   - Keyword penting (pajak, gaji, dll) tidak di-stem\")\n",
    "print(\"   - Slang sudah dinormalisasi (jika menggunakan indoNLP)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367dd406",
   "metadata": {},
   "source": [
    "## 3. Lihat Detail Hasil Preprocessing (Opsional)\n",
    "\n",
    "Melihat hasil setiap tahap preprocessing untuk beberapa contoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32ada77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===========================================================================\n",
      "üìä STATISTIK PREPROCESSING LENGKAP\n",
      "===========================================================================\n",
      "\n",
      "üìà JUMLAH DATA:\n",
      "   Data awal (raw)                     :      2,196 komentar\n",
      "   Setelah filter konteks & noise      :        641 komentar\n",
      "   Data final valid                    :        641 komentar\n",
      "   Persentase data valid               :      29.19%\n",
      "   Data terbuang                       :      1,555 komentar\n",
      "\n",
      "üìù RATA-RATA PANJANG TEKS (karakter):\n",
      "   Teks asli                           :     209.97\n",
      "   Teks final (preprocessed)           :     117.70\n",
      "   Reduksi                             :      43.94%\n",
      "\n",
      "üìÑ RATA-RATA JUMLAH KATA:\n",
      "   Teks asli                           :      32.66 kata\n",
      "   Teks final (preprocessed)           :      17.64 kata\n",
      "   Reduksi                             :      45.98%\n",
      "\n",
      "üîÑ RATA-RATA TOKENS PER TAHAP:\n",
      "   Setelah Tokenisasi (3a)             :      29.84 tokens\n",
      "   Setelah Negation Tagging (3b)       :      29.13 tokens\n",
      "   Setelah Stemming (4)                :      29.13 tokens\n",
      "   Setelah Stopword Removal (5)        :      17.64 tokens\n",
      "   Reduksi total tokens                :      40.87%\n",
      "\n",
      "üè∑Ô∏è  FITUR KHUSUS:\n",
      "   Total TIDAK_xxx tags                :        457 tag\n",
      "   Total _EMOJI_xxx tags               :          0 tag\n",
      "   Keyword yang di-protect             :         43 kata\n",
      "   Kata negasi yang di-protect         :         11 kata\n",
      "   Keyword yang di-protect             :         43 kata\n",
      "   Kata negasi yang di-protect         :         11 kata\n",
      "\n",
      "===========================================================================\n",
      "‚úÖ Preprocessing berhasil!\n",
      "üíæ File tersimpan: data_preprocessed.csv\n",
      "\n",
      "üìå PIPELINE YANG DITERAPKAN:\n",
      "   1. ‚úÖ Filter Konteks & Noise (Strategi LONGGAR)\n",
      "   2. ‚úÖ Cleaning (URL, HTML, Emoji‚ÜíTag, Slang*, Elongasi*)\n",
      "   2. ‚úÖ Cleaning (URL, HTML, Hapus Emoji, Slang*, Elongasi*)\n",
      "   4. ‚úÖ Stemming (skip keyword & tag)\n",
      "   5. ‚úÖ Stopword Removal (preserve negasi, emoji, keyword)\n",
      "   5. ‚úÖ Stopword Removal (preserve negasi & keyword)\n",
      "\n",
      "üìå PERBAIKAN UTAMA:\n",
      "   ‚úÖ Urutan BENAR: Stemming dilakukan SEBELUM Stopword Removal\n",
      "   ‚úÖ Kata negasi DIPERTAHANKAN dan di-tag dengan kata berikutnya\n",
      "   ‚úÖ Emoji diubah menjadi tag sentimen sederhana\n",
      "   ‚úÖ Emoji DIHAPUS dari data (tidak digunakan sebagai fitur)\n",
      "   ‚úÖ Keyword topik penting TIDAK di-stem untuk preserve makna\n",
      "   ‚≠ê Slang & elongasi dinormalisasi (jika indoNLP terinstall)\n",
      "\n",
      "üìù CATATAN UNTUK FASE MODELLING:\n",
      "   ‚ö†Ô∏è  Gunakan ngram_range=(1, 2) saat TF-IDF Vectorization\n"
     ]
    }
   ],
   "source": [
    "# Hitung statistik lengkap\n",
    "total_awal = len(df)\n",
    "total_setelah_filter = len(df)\n",
    "total_final = len(df_final)\n",
    "persentase_valid = (total_final / total_awal * 100) if total_awal > 0 else 0\n",
    "\n",
    "# Hitung rata-rata panjang teks\n",
    "avg_len_asli = df_final['Teks_Komentar'].str.len().mean()\n",
    "avg_len_final = df_final['teks_final'].str.len().mean()\n",
    "\n",
    "# Hitung rata-rata jumlah kata\n",
    "avg_words_asli = df_final['Teks_Komentar'].str.split().str.len().mean()\n",
    "avg_words_final = df_final['teks_final'].str.split().str.len().mean()\n",
    "\n",
    "# Hitung statistik tokens per tahap\n",
    "avg_tokens_tahap2 = df_final['tokens_tahap2'].apply(len).mean()\n",
    "avg_tokens_tahap2_5 = df_final['tokens_tahap2_5'].apply(len).mean()\n",
    "avg_tokens_tahap3 = df_final['tokens_tahap3'].apply(len).mean()\n",
    "avg_tokens_tahap4 = df_final['tokens_tahap4'].apply(len).mean()\n",
    "\n",
    "# Hitung berapa banyak negasi yang di-tag\n",
    "def count_negation_tags(tokens):\n",
    "    return sum(1 for token in tokens if token.startswith('TIDAK_'))\n",
    "\n",
    "total_negation_tags = df_final['tokens_tahap4'].apply(count_negation_tags).sum()\n",
    "\n",
    "# Emoji sudah dihapus, tidak perlu hitung emoji tags\n",
    "\n",
    "print(\"=\" * 75)\n",
    "print(\"üìä STATISTIK PREPROCESSING LENGKAP\")\n",
    "print(\"=\" * 75)\n",
    "\n",
    "print(f\"\\nüìà JUMLAH DATA:\")\n",
    "print(f\"   {'Data awal (raw)':<35} : {total_awal:>10,} komentar\")\n",
    "print(f\"   {'Setelah filter konteks & noise':<35} : {total_setelah_filter:>10,} komentar\")\n",
    "print(f\"   {'Data final valid':<35} : {total_final:>10,} komentar\")\n",
    "print(f\"   {'Persentase data valid':<35} : {persentase_valid:>10.2f}%\")\n",
    "print(f\"   {'Data terbuang':<35} : {total_awal - total_final:>10,} komentar\")\n",
    "\n",
    "print(f\"\\nüìù RATA-RATA PANJANG TEKS (karakter):\")\n",
    "print(f\"   {'Teks asli':<35} : {avg_len_asli:>10.2f}\")\n",
    "print(f\"   {'Teks final (preprocessed)':<35} : {avg_len_final:>10.2f}\")\n",
    "print(f\"   {'Reduksi':<35} : {((avg_len_asli - avg_len_final) / avg_len_asli * 100):>10.2f}%\")\n",
    "\n",
    "print(f\"\\nüìÑ RATA-RATA JUMLAH KATA:\")\n",
    "print(f\"   {'Teks asli':<35} : {avg_words_asli:>10.2f} kata\")\n",
    "print(f\"   {'Teks final (preprocessed)':<35} : {avg_words_final:>10.2f} kata\")\n",
    "print(f\"   {'Reduksi':<35} : {((avg_words_asli - avg_words_final) / avg_words_asli * 100):>10.2f}%\")\n",
    "\n",
    "print(f\"\\nüîÑ RATA-RATA TOKENS PER TAHAP:\")\n",
    "print(f\"   {'Setelah Tokenisasi (3a)':<35} : {avg_tokens_tahap2:>10.2f} tokens\")\n",
    "print(f\"   {'Setelah Negation Tagging (3b)':<35} : {avg_tokens_tahap2_5:>10.2f} tokens\")\n",
    "print(f\"   {'Setelah Stemming (4)':<35} : {avg_tokens_tahap3:>10.2f} tokens\")\n",
    "print(f\"   {'Setelah Stopword Removal (5)':<35} : {avg_tokens_tahap4:>10.2f} tokens\")\n",
    "print(f\"   {'Reduksi total tokens':<35} : {((avg_tokens_tahap2 - avg_tokens_tahap4) / avg_tokens_tahap2 * 100):>10.2f}%\")\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è  FITUR KHUSUS:\")\n",
    "print(f\"   {'Total TIDAK_xxx tags':<35} : {total_negation_tags:>10,} tag\")\n",
    "print(f\"   {'Keyword yang di-protect':<35} : {len(KEYWORD_SET):>10,} kata\")\n",
    "print(f\"   {'Kata negasi yang di-protect':<35} : {len(NEGASI_WORDS):>10,} kata\")\n",
    "\n",
    "print(f\"   {'Keyword yang di-protect':<35} : {len(KEYWORD_SET):>10,} kata\")\n",
    "print(f\"   {'Kata negasi yang di-protect':<35} : {len(NEGASI_WORDS):>10,} kata\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 75)\n",
    "print(\"‚úÖ Preprocessing berhasil!\")\n",
    "print(f\"üíæ File tersimpan: {output_file}\")\n",
    "print(\"\\nüìå PIPELINE YANG DITERAPKAN:\")\n",
    "print(\"   1. ‚úÖ Filter Konteks & Noise (Strategi LONGGAR)\")\n",
    "print(\"   2. ‚úÖ Cleaning (URL, HTML, Emoji‚ÜíTag, Slang*, Elongasi*)\")\n",
    "print(\"   2. ‚úÖ Cleaning (URL, HTML, Hapus Emoji, Slang*, Elongasi*)\")\n",
    "print(\"   4. ‚úÖ Stemming (skip keyword & tag)\")\n",
    "print(\"   5. ‚úÖ Stopword Removal (preserve negasi, emoji, keyword)\")\n",
    "print(\"   5. ‚úÖ Stopword Removal (preserve negasi & keyword)\")\n",
    "print(\"\\nüìå PERBAIKAN UTAMA:\")\n",
    "print(\"   ‚úÖ Urutan BENAR: Stemming dilakukan SEBELUM Stopword Removal\")\n",
    "print(\"   ‚úÖ Kata negasi DIPERTAHANKAN dan di-tag dengan kata berikutnya\")\n",
    "print(\"   ‚úÖ Emoji diubah menjadi tag sentimen sederhana\")\n",
    "print(\"   ‚úÖ Emoji DIHAPUS dari data (tidak digunakan sebagai fitur)\")\n",
    "print(\"   ‚úÖ Keyword topik penting TIDAK di-stem untuk preserve makna\")\n",
    "print(\"   ‚≠ê Slang & elongasi dinormalisasi (jika indoNLP terinstall)\")\n",
    "print(\"\\nüìù CATATAN UNTUK FASE MODELLING:\")\n",
    "print(\"   ‚ö†Ô∏è  Gunakan ngram_range=(1, 2) saat TF-IDF Vectorization\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
