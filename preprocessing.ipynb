{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19494fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhzain\\AppData\\Local\\Temp\\ipykernel_24928\\1846252205.py:8: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import demoji\n",
    "\n",
    "# Download emoji data (run once)\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2385c25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total komentar: 1072\n",
      "\n",
      "Contoh data awal:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Video_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Teks_Komentar",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "text_normalized_temp",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e0d7e5c8-5d3a-407b-89ac-53d414196771",
       "rows": [
        [
         "0",
         "MIo4tGN11j0",
         "Sempat mikir mau pindah ke negara sebelah, ngeapply citizenship. Tapi yah, aku cinta negara dan harapanku semoga negara ini bisa sembuh secepatnya.",
         "sempat mikir mau pindah ke negara sebelah, ngeapply citizenship. tapi yah, aku cinta negara dan harapanku semoga negara ini bisa sembuh secepatnya."
        ],
        [
         "1",
         "MIo4tGN11j0",
         "Kalo kabur mau kemana ke Singapur ,emang di Singapur tinggal dimana rakyat Singapur aja nggak punya rumah mereka masih nyicilüòÇ\"katanya negara kaya Singapur \"",
         "kalau kabur mau kemana ke singapur ,emang di singapur tinggal dimana rakyat singapur saja tidak punya rumah mereka masih nyicilüòÇ\"katanya negara kaya singapur \""
        ],
        [
         "2",
         "MIo4tGN11j0",
         "Klo sudah gelap susah terangnya lebih baik bubar",
         "kalau sudah gelap susah terangnya lebih baik bubar"
        ],
        [
         "3",
         "MIo4tGN11j0",
         "Siap siap ente motivator Indonesia gelap...dapet cipratan Marcela Santoso atau engga",
         "siap siap ente motivator indonesia gelap...dapet cipratan marcela santoso atau tidak"
        ],
        [
         "4",
         "MIo4tGN11j0",
         "Diskusi yg segar,menarik dan bermutu... Antara Raymond dan FS saling mengisi tanpa dominan atau menonjolkan diri... Hebatnya lg mereka berdua upgrade dgn  zaman saat ini sehingga topiknya mnjd menarik,tdk membosankan... Seandainya Indonesia memiliki 30% rakyat secerdas mereka berdua, Insyaallah Indonesia pasti maju... Semoga sehat selalu mereka berdua.... Aamiin yra",
         "diskusi yang segar,menarik dan bermutu... antara raymond dan fs saling mengisi tanpa dominan atau menonjolkan diri... hebatnya lagi mereka berdua upgrade dengan zaman saat ini sehingga topiknya menjadi menarik,tdk membosankan... seandainya indonesia memiliki 30% rakyat secerdas mereka berdua, insyaallah indonesia pasti maju... semoga sehat selalu mereka berdua.... aamiin yra"
        ]
       ],
       "shape": {
        "columns": 3,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video_ID</th>\n",
       "      <th>Teks_Komentar</th>\n",
       "      <th>text_normalized_temp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>Sempat mikir mau pindah ke negara sebelah, nge...</td>\n",
       "      <td>sempat mikir mau pindah ke negara sebelah, nge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>Kalo kabur mau kemana ke Singapur ,emang di Si...</td>\n",
       "      <td>kalau kabur mau kemana ke singapur ,emang di s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>Klo sudah gelap susah terangnya lebih baik bubar</td>\n",
       "      <td>kalau sudah gelap susah terangnya lebih baik b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>Siap siap ente motivator Indonesia gelap...dap...</td>\n",
       "      <td>siap siap ente motivator indonesia gelap...dap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>Diskusi yg segar,menarik dan bermutu... Antara...</td>\n",
       "      <td>diskusi yang segar,menarik dan bermutu... anta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Video_ID                                      Teks_Komentar  \\\n",
       "0  MIo4tGN11j0  Sempat mikir mau pindah ke negara sebelah, nge...   \n",
       "1  MIo4tGN11j0  Kalo kabur mau kemana ke Singapur ,emang di Si...   \n",
       "2  MIo4tGN11j0   Klo sudah gelap susah terangnya lebih baik bubar   \n",
       "3  MIo4tGN11j0  Siap siap ente motivator Indonesia gelap...dap...   \n",
       "4  MIo4tGN11j0  Diskusi yg segar,menarik dan bermutu... Antara...   \n",
       "\n",
       "                                text_normalized_temp  \n",
       "0  sempat mikir mau pindah ke negara sebelah, nge...  \n",
       "1  kalau kabur mau kemana ke singapur ,emang di s...  \n",
       "2  kalau sudah gelap susah terangnya lebih baik b...  \n",
       "3  siap siap ente motivator indonesia gelap...dap...  \n",
       "4  diskusi yang segar,menarik dan bermutu... anta...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_filtered.csv')\n",
    "print(f\"Total komentar: {len(df)}\")\n",
    "print(\"\\nContoh data awal:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e655cd7",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Teks\n",
    "\n",
    "Melakukan preprocessing data dengan 5 tahap terpisah yang modular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e080c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INISIALISASI PREPROCESSING TOOLS\n",
      "============================================================\n",
      "‚úÖ Sastrawi Stemmer berhasil dimuat\n",
      "‚ö†Ô∏è  WARNING: Library indoNLP tidak ditemukan.\n",
      "   Silakan instal dengan: pip install indonlp\n",
      "   Preprocessing akan tetap berjalan tanpa indoNLP.\n",
      "‚úÖ KEYWORD dimuat: 37 frasa ‚Üí 43 kata unik\n",
      "‚úÖ Stopwords berhasil dimuat:\n",
      "   - Indonesia       : 757 kata\n",
      "   - Inggris         : 198 kata\n",
      "   - Gaul + Rojak    : 51 kata\n",
      "   - TOTAL           : 985 kata (setelah exclude negasi & keyword)\n",
      "   - Negasi (PROTECT): {'bukan', 'nggak', 'ga', 'enggak', 'jangan', 'ndak', 'tanpa', 'gak', 'belum', 'engga', 'tidak'}\n",
      "   - Keyword (PROTECT): 43 kata\n",
      "\n",
      "============================================================\n",
      "‚úÖ Semua fungsi preprocessing pipeline sudah siap!\n",
      "\n",
      "üìã URUTAN PIPELINE:\n",
      "   Tahap 1: Filter Konteks & Noise\n",
      "   Tahap 2: Cleaning (URL, HTML, Hapus Emoji, Slang, Elongasi, Tanda Baca)\n",
      "   Tahap 3: Tokenisasi + Negation Normalization (ga/gak/enggak ‚Üí tidak)\n",
      "   Tahap 4: Stemming (skip keyword & 'tidak')\n",
      "   Tahap 5: Stopword Removal (preserve negasi & keyword)\n",
      "   Tahap 6: Gabung Kembali\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import demoji\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- 1. Inisialisasi Alat (Sekali saja) ---\n",
    "print(\"=\"*60)\n",
    "print(\"INISIALISASI PREPROCESSING TOOLS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Impor Stemmer dari Sastrawi\n",
    "try:\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    print(\"‚úÖ Sastrawi Stemmer berhasil dimuat\")\n",
    "    factory = StemmerFactory()\n",
    "    stemmer_sastrawi = factory.create_stemmer()\n",
    "except ImportError:\n",
    "    print(\"‚ùå ERROR: Library Sastrawi tidak ditemukan.\")\n",
    "    print(\"   Silakan instal dengan: pip install Sastrawi\")\n",
    "    raise\n",
    "\n",
    "# Import indoNLP untuk preprocessing slang dan elongation\n",
    "try:\n",
    "    from indonlp.preprocessing import replace_slang, replace_word_elongation\n",
    "    print(\"‚úÖ indoNLP preprocessing berhasil dimuat\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  WARNING: Library indoNLP tidak ditemukan.\")\n",
    "    print(\"   Silakan instal dengan: pip install indonlp\")\n",
    "    print(\"   Preprocessing akan tetap berjalan tanpa indoNLP.\")\n",
    "    replace_slang = lambda x: x\n",
    "    replace_word_elongation = lambda x: x\n",
    "\n",
    "# --- 2. Buat Daftar KEYWORD untuk Filter (dari Cell 4) ---\n",
    "KEYWORD = [\n",
    "    'kabur', 'pindah negara', 'leave indo', 'pajak', 'gaji', 'umr', \n",
    "    'biaya hidup', 'korupsi', 'sandwich', 'luar negeri', \n",
    "    'paspor', 'warga negara', 'singapur', 'singapore', 'australia', 'aussie', 'jepang', 'eropa', 'wni',\n",
    "    'capek', 'lelah', 'pemerintah', 'mending', 'percuma', 'beban', \n",
    "    'suram', 'males', 'ga jelas', 'ga ada harapan', 'nyicil', 'susah',\n",
    "    'setuju', 'sulit', 'stres', 'politik', 'birokrasi', 'konoha'\n",
    "]\n",
    "\n",
    "# Buat set untuk pencarian cepat saat stemming\n",
    "KEYWORD_SET = set()\n",
    "for keyword in KEYWORD:\n",
    "    # Pecah multi-word keywords jadi single word\n",
    "    KEYWORD_SET.update(keyword.lower().split())\n",
    "\n",
    "print(f\"‚úÖ KEYWORD dimuat: {len(KEYWORD)} frasa ‚Üí {len(KEYWORD_SET)} kata unik\")\n",
    "\n",
    "# --- 3. Buat Daftar Stopwords Gabungan ---\n",
    "stop_words_indo = set(stopwords.words('indonesian'))\n",
    "stop_words_eng = set(stopwords.words('english'))\n",
    "\n",
    "# Stopwords gaul dan kata rojak (bahasa Inggris umum)\n",
    "custom_stopwords_gaul = {\n",
    "    'yg', 'dg', 'rt', 'dgn', 'ny', 'd', 'klo', 'kalo', 'amp', 'biar', 'bkn', 'na', \n",
    "    'nya', 'nih', 'sih', 'si', 'tau', 'tuh', 'utk', 'ya', 'gaes',\n",
    "    'bang', 'bro', 'sob', 'gw', 'gua', 'lu', 'lo', 'wkwk', 'haha', 'wkwkwk', \n",
    "    'amin', 'amiin', 'aamiin', 'yuk', 'dong', 'deh', 'kok',\n",
    "    # Kata rojak (bahasa Inggris umum yang tidak bisa di-stem)\n",
    "    'government', 'tax', 'salary', 'system', 'netizen', 'the', 'and', 'or', 'in', 'of', 'to', 'is', 'for'\n",
    "}\n",
    "\n",
    "# --- 4. Daftar Kata NEGASI yang TIDAK BOLEH dihapus ---\n",
    "NEGASI_WORDS = {\n",
    "    'tidak', 'bukan', 'jangan', 'ga', 'gak', 'enggak', 'nggak', 'ndak', 'engga', 'belum', 'tanpa'\n",
    "}\n",
    "\n",
    "# Gabungkan semua stopwords, KECUALI kata negasi dan keyword penting\n",
    "stop_words_final = (stop_words_indo.union(stop_words_eng, custom_stopwords_gaul)) - NEGASI_WORDS - KEYWORD_SET\n",
    "\n",
    "print(\"‚úÖ Stopwords berhasil dimuat:\")\n",
    "print(f\"   - Indonesia       : {len(stop_words_indo)} kata\")\n",
    "print(f\"   - Inggris         : {len(stop_words_eng)} kata\")\n",
    "print(f\"   - Gaul + Rojak    : {len(custom_stopwords_gaul)} kata\")\n",
    "print(f\"   - TOTAL           : {len(stop_words_final)} kata (setelah exclude negasi & keyword)\")\n",
    "print(f\"   - Negasi (PROTECT): {NEGASI_WORDS}\")\n",
    "print(f\"   - Keyword (PROTECT): {len(KEYWORD_SET)} kata\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "\n",
    "# --- 5. Definisi Fungsi Preprocessing Pipeline ---\n",
    "\n",
    "def tahap1_cleaning(teks):\n",
    "    \"\"\"\n",
    "    Tahap 2: Membersihkan teks mentah dari URL, HTML, emoji,\n",
    "    normalisasi slang, elongasi, tanda baca, dan angka.\n",
    "    \n",
    "    DENGAN indoNLP untuk slang dan elongation!\n",
    "    \"\"\"\n",
    "    if pd.isna(teks):\n",
    "        return \"\"\n",
    "    \n",
    "    teks = str(teks)\n",
    "    \n",
    "    # 1. Hapus URL\n",
    "    teks = re.sub(r'http\\S+|www\\.\\S+', '', teks)\n",
    "    \n",
    "    # 2. Hapus HTML tags\n",
    "    teks = re.sub(r'<.*?>', '', teks)\n",
    "    \n",
    "    # 3. Hapus SEMUA emoji (tanpa konversi ke tag)\n",
    "    teks = demoji.replace(teks, '')\n",
    "    \n",
    "    # 4. Normalisasi Unicode (font aneh)\n",
    "    teks = unicodedata.normalize('NFKD', teks)\n",
    "    \n",
    "    # 5. Lowercase\n",
    "    teks = teks.lower()\n",
    "    \n",
    "    # 6. Ganti slang menggunakan indoNLP\n",
    "    teks = replace_slang(teks)\n",
    "    \n",
    "    # 7. Ganti elongasi (kata berulang) menggunakan indoNLP\n",
    "    teks = replace_word_elongation(teks)\n",
    "    \n",
    "    # 8. Hapus tanda baca dan angka (sisakan huruf dan spasi)\n",
    "    teks = re.sub(r'[^a-z\\s]', ' ', teks)\n",
    "    \n",
    "    # 9. Hapus spasi berlebih\n",
    "    teks = re.sub(r'\\s+', ' ', teks).strip()\n",
    "    \n",
    "    return teks\n",
    "\n",
    "\n",
    "def tahap2_tokenisasi(teks):\n",
    "    \"\"\"\n",
    "    Tahap 3a: Tokenisasi - Memecah teks menjadi list kata-kata.\n",
    "    \"\"\"\n",
    "    if not teks or pd.isna(teks):\n",
    "        return []\n",
    "    \n",
    "    # Pecah berdasarkan spasi\n",
    "    tokens = teks.split()\n",
    "    \n",
    "    # Filter kata yang terlalu pendek (< 3 karakter), KECUALI kata negasi pendek\n",
    "    tokens = [kata for kata in tokens if len(kata) >= 3 or kata in {'ga', 'gak'}]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tahap2_5_negation_tagging(tokens):\n",
    "    \"\"\"\n",
    "    Tahap 3b: Negation Handling - Normalisasi semua kata negasi menjadi 'tidak'.\n",
    "    \n",
    "    Semua variasi negasi (ga, gak, enggak, dll) ‚Üí 'tidak'\n",
    "    Tetap sebagai 2 token terpisah, TF-IDF bigram akan menangkap pola.\n",
    "    \n",
    "    Contoh: ['gaji', 'ga', 'naik'] -> ['gaji', 'tidak', 'naik']\n",
    "    \"\"\"\n",
    "    if not tokens or len(tokens) == 0:\n",
    "        return []\n",
    "    \n",
    "    tokens_normalized = []\n",
    "    \n",
    "    for token in tokens:\n",
    "        # Jika kata ini adalah negasi, ganti dengan 'tidak' yang standar\n",
    "        if token in NEGASI_WORDS:\n",
    "            tokens_normalized.append('tidak')\n",
    "        else:\n",
    "            tokens_normalized.append(token)\n",
    "    \n",
    "    return tokens_normalized\n",
    "\n",
    "\n",
    "def tahap4_stemming(tokens):\n",
    "    \"\"\"\n",
    "    Tahap 4: Stemming - Mengubah kata ke bentuk dasar.\n",
    "    \n",
    "    PENTING: Skip stemming untuk:\n",
    "    - Kata yang ada di KEYWORD_SET (kata kunci penting topik)\n",
    "    - Kata 'tidak' (kata negasi standar)\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    tokens_stemmed = []\n",
    "    for kata in tokens:\n",
    "        # Jika kata adalah keyword penting, JANGAN di-stem\n",
    "        if kata in KEYWORD_SET:\n",
    "            tokens_stemmed.append(kata)\n",
    "        # Jangan stem kata 'tidak'\n",
    "        elif kata == 'tidak':\n",
    "            tokens_stemmed.append(kata)\n",
    "        else:\n",
    "            # Kata biasa, stem seperti biasa\n",
    "            tokens_stemmed.append(stemmer_sastrawi.stem(kata))\n",
    "    \n",
    "    return tokens_stemmed\n",
    "\n",
    "\n",
    "def tahap3_stopword_removal(tokens):\n",
    "    \"\"\"\n",
    "    Tahap 5: Hapus Stopwords dari list tokens.\n",
    "    \n",
    "    Dilakukan SETELAH stemming.\n",
    "    JANGAN hapus: kata 'tidak' dan keyword penting.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    tokens_bersih = []\n",
    "    for kata in tokens:\n",
    "        # Jangan hapus kata 'tidak' (negasi)\n",
    "        if kata == 'tidak':\n",
    "            tokens_bersih.append(kata)\n",
    "        # Jangan hapus keyword penting\n",
    "        elif kata in KEYWORD_SET:\n",
    "            tokens_bersih.append(kata)\n",
    "        # Hapus stopwords biasa\n",
    "        elif kata not in stop_words_final:\n",
    "            tokens_bersih.append(kata)\n",
    "    \n",
    "    return tokens_bersih\n",
    "\n",
    "\n",
    "def tahap5_gabung_kembali(tokens):\n",
    "    \"\"\"\n",
    "    Tahap 6: Gabungkan tokens menjadi teks final.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return \"\"\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"‚úÖ Semua fungsi preprocessing pipeline sudah siap!\")\n",
    "print(\"\\nüìã URUTAN PIPELINE:\")\n",
    "print(\"   Tahap 1: Filter Konteks & Noise\")\n",
    "print(\"   Tahap 2: Cleaning (URL, HTML, Hapus Emoji, Slang, Elongasi, Tanda Baca)\")\n",
    "print(\"   Tahap 3: Tokenisasi + Negation Normalization (ga/gak/enggak ‚Üí tidak)\")\n",
    "print(\"   Tahap 4: Stemming (skip keyword & 'tidak')\")\n",
    "print(\"   Tahap 5: Stopword Removal (preserve negasi & keyword)\")\n",
    "print(\"   Tahap 6: Gabung Kembali\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beeb1d1",
   "metadata": {},
   "source": [
    "### Tahap 1: Cleaning\n",
    "\n",
    "Membersihkan teks dari URL, HTML, emoji, tanda baca, dan angka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "66bba9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 2/6: CLEANING\n",
      "============================================================\n",
      "Membersihkan URL, HTML, emoji‚Üítag, slang, elongasi, tanda baca...\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil cleaning:\n",
      "ASLI  : Sempat mikir mau pindah ke negara sebelah, ngeapply citizenship. Tapi yah, aku c...\n",
      "CLEAN : sempat mikir mau pindah ke negara sebelah ngeapply citizenship tapi yah aku cint...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 2/6: CLEANING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Membersihkan URL, HTML, emoji‚Üítag, slang, elongasi, tanda baca...\")\n",
    "\n",
    "df['teks_tahap1'] = df['text_normalized_temp'].apply(tahap1_cleaning)\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"\\nContoh hasil cleaning:\")\n",
    "print(f\"ASLI  : {df['Teks_Komentar'].iloc[0][:80]}...\")\n",
    "print(f\"CLEAN : {df['teks_tahap1'].iloc[0][:80]}...\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4906f4a",
   "metadata": {},
   "source": [
    "### Tahap 3: Tokenisasi & Negation Tagging\n",
    "\n",
    "Memecah teks menjadi kata-kata dan menggabungkan kata negasi dengan kata berikutnya"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d5e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 3/6: TOKENISASI & NEGATION NORMALIZATION\n",
      "============================================================\n",
      "Memecah teks menjadi kata-kata...\n",
      "‚úÖ Tokenisasi selesai!\n",
      "\n",
      "Contoh hasil tokenisasi:\n",
      "TEKS   : sempat mikir mau pindah ke negara sebelah ngeapply citizensh...\n",
      "TOKENS : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'sebelah', 'ngeapply', 'citizenship']...\n",
      "JUMLAH : 21 kata\n",
      "\n",
      "Menormalisasi kata negasi (ga/gak/enggak ‚Üí tidak)...\n",
      "‚úÖ Negation normalization selesai!\n",
      "\n",
      "Contoh hasil:\n",
      "SEBELUM : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'sebelah', 'ngeapply', 'citizenship']...\n",
      "SESUDAH : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'sebelah', 'ngeapply', 'citizenship']...\n",
      "\n",
      "üí° Contoh: 'gaji ga naik' ‚Üí 'gaji tidak naik'\n",
      "   TF-IDF bigram akan menangkap: 'tidak naik', 'tidak bagus', dll\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 3/6: TOKENISASI & NEGATION NORMALIZATION\")\n",
    "print(\"=\"*60)\n",
    "print(\"Memecah teks menjadi kata-kata...\")\n",
    "\n",
    "# Tahap 3a: Tokenisasi\n",
    "df['tokens_tahap2'] = df['teks_tahap1'].apply(tahap2_tokenisasi)\n",
    "\n",
    "print(\"‚úÖ Tokenisasi selesai!\")\n",
    "print(f\"\\nContoh hasil tokenisasi:\")\n",
    "tokens_contoh = df['tokens_tahap2'].iloc[0]\n",
    "print(f\"TEKS   : {df['teks_tahap1'].iloc[0][:60]}...\")\n",
    "print(f\"TOKENS : {tokens_contoh[:8]}...\")\n",
    "print(f\"JUMLAH : {len(tokens_contoh)} kata\")\n",
    "\n",
    "# Tahap 3b: Negation Normalization\n",
    "print(\"\\nMenormalisasi kata negasi (ga/gak/enggak ‚Üí tidak)...\")\n",
    "df['tokens_tahap2_5'] = df['tokens_tahap2'].apply(tahap2_5_negation_tagging)\n",
    "\n",
    "print(\"‚úÖ Negation normalization selesai!\")\n",
    "print(f\"\\nContoh hasil:\")\n",
    "tokens_before = df['tokens_tahap2'].iloc[0]\n",
    "tokens_after = df['tokens_tahap2_5'].iloc[0]\n",
    "print(f\"SEBELUM : {tokens_before[:8]}...\")\n",
    "print(f\"SESUDAH : {tokens_after[:8]}...\")\n",
    "print(f\"\\nüí° Contoh: 'gaji ga naik' ‚Üí 'gaji tidak naik'\")\n",
    "print(\"   TF-IDF bigram akan menangkap: 'tidak naik', 'tidak bagus', dll\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d010b376",
   "metadata": {},
   "source": [
    "### Tahap 4: Stemming\n",
    "\n",
    "Mengubah kata ke bentuk dasar (SKIP keyword penting dan tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2233508f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 4/6: STEMMING\n",
      "============================================================\n",
      "Mengubah kata ke bentuk dasar...\n",
      "‚ö†Ô∏è  Proses ini lambat, harap sabar...\n",
      "üí° Keyword penting & tag (TIDAK_, _EMOJI_) akan di-skip\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil stemming:\n",
      "SEBELUM : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'sebelah']\n",
      "SESUDAH : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'belah']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 4/6: STEMMING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Mengubah kata ke bentuk dasar...\")\n",
    "print(\"‚ö†Ô∏è  Proses ini lambat, harap sabar...\")\n",
    "print(\"üí° Keyword penting & tag (TIDAK_, _EMOJI_) akan di-skip\")\n",
    "\n",
    "df['tokens_tahap3'] = df['tokens_tahap2_5'].apply(tahap4_stemming)\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"\\nContoh hasil stemming:\")\n",
    "tokens_before = df['tokens_tahap2_5'].iloc[0]\n",
    "tokens_after = df['tokens_tahap3'].iloc[0]\n",
    "print(f\"SEBELUM : {tokens_before[:6]}\")\n",
    "print(f\"SESUDAH : {tokens_after[:6]}\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c84efc2",
   "metadata": {},
   "source": [
    "### Tahap 5: Stopword Removal\n",
    "\n",
    "Menghapus stopwords SETELAH stemming (preserve negasi, emoji, dan keyword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "553bb734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 5/6: STOPWORD REMOVAL\n",
      "============================================================\n",
      "Menghapus stopwords (preserve: negasi, emoji tag, keyword)...\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil stopword removal:\n",
      "SEBELUM : ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'belah', 'ngeapply', 'citizenship']...\n",
      "SESUDAH : ['mikir', 'pindah', 'negara', 'belah', 'ngeapply', 'citizenship', 'yah', 'cinta']...\n",
      "JUMLAH  : 21 kata ‚Üí 14 kata\n",
      "REDUKSI : 7 kata dihapus\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 5/6: STOPWORD REMOVAL\")\n",
    "print(\"=\"*60)\n",
    "print(\"Menghapus stopwords (preserve: negasi, emoji tag, keyword)...\")\n",
    "\n",
    "df['tokens_tahap4'] = df['tokens_tahap3'].apply(tahap3_stopword_removal)\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"\\nContoh hasil stopword removal:\")\n",
    "tokens_before = df['tokens_tahap3'].iloc[0]\n",
    "tokens_after = df['tokens_tahap4'].iloc[0]\n",
    "print(f\"SEBELUM : {tokens_before[:8]}...\")\n",
    "print(f\"SESUDAH : {tokens_after[:8]}...\")\n",
    "print(f\"JUMLAH  : {len(tokens_before)} kata ‚Üí {len(tokens_after)} kata\")\n",
    "print(f\"REDUKSI : {len(tokens_before) - len(tokens_after)} kata dihapus\")\n",
    "print(\"=\"*60)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c54c741",
   "metadata": {},
   "source": [
    "### Tahap 6: Gabung Kembali & Simpan\n",
    "\n",
    "Menggabungkan tokens menjadi teks final dan menyimpan hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f266a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 6/6: GABUNG KEMBALI & SIMPAN\n",
      "============================================================\n",
      "Menggabungkan tokens menjadi teks final...\n",
      "‚úÖ Selesai!\n",
      "\n",
      "üìä Ringkasan:\n",
      "   Total komentar valid     : 1,072\n",
      "   Komentar kosong terbuang : 0\n",
      "\n",
      "Menggabungkan dengan label sentiment...\n",
      "‚úÖ Sentiment berhasil digabungkan!\n",
      "\n",
      "Distribusi sentiment:\n",
      "sentiment\n",
      "Positif    695\n",
      "Netral     204\n",
      "Negatif    181\n",
      "Name: count, dtype: int64\n",
      "\n",
      "üíæ Data berhasil disimpan ke 'data_preprocessed.csv'\n",
      "\n",
      "üìã Contoh hasil akhir:\n",
      "\n",
      "   [1] SENTIMENT: Negatif\n",
      "       ASLI     : Sempat mikir mau pindah ke negara sebelah, ngeapply citizens...\n",
      "       FINAL    : mikir pindah negara belah ngeapply citizenship yah cinta neg...\n",
      "\n",
      "   [2] SENTIMENT: Negatif\n",
      "       ASLI     : Kalo kabur mau kemana ke Singapur ,emang di Singapur tinggal...\n",
      "       FINAL    : kabur singapur emang singapur tinggal rakyat singapur tidak ...\n",
      "\n",
      "   [3] SENTIMENT: Positif\n",
      "       ASLI     : Klo sudah gelap susah terangnya lebih baik bubar...\n",
      "       FINAL    : gelap susah terang bubar...\n",
      "\n",
      "============================================================\n",
      "üéâ PREPROCESSING SELESAI!\n",
      "============================================================\n",
      "\n",
      "üìå PIPELINE YANG DITERAPKAN:\n",
      "   ‚úÖ Tahap 1: Filter Konteks & Noise (Strategi LONGGAR)\n",
      "   ‚úÖ Tahap 2: Cleaning (URL, HTML, Hapus Emoji, Slang, Elongasi)\n",
      "   ‚úÖ Tahap 3: Tokenisasi + Negation Normalization (ga/gak ‚Üí tidak)\n",
      "   ‚úÖ Tahap 4: Stemming (skip keyword & 'tidak')\n",
      "   ‚úÖ Tahap 5: Stopword Removal (preserve negasi & keyword)\n",
      "   ‚úÖ Tahap 6: Gabung Kembali + Merge Sentiment Label\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 6/6: GABUNG KEMBALI & SIMPAN\")\n",
    "print(\"=\"*60)\n",
    "print(\"Menggabungkan tokens menjadi teks final...\")\n",
    "\n",
    "df['teks_final'] = df['tokens_tahap4'].apply(tahap5_gabung_kembali)\n",
    "\n",
    "# Buang baris dengan teks kosong setelah preprocessing\n",
    "df_final = df[df['teks_final'].str.strip() != ''].copy()\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"\\nüìä Ringkasan:\")\n",
    "print(f\"   Total komentar valid     : {len(df_final):,}\")\n",
    "print(f\"   Komentar kosong terbuang : {len(df) - len(df_final):,}\")\n",
    "\n",
    "# Load dan gabung dengan sentiment dari dataset_labeled.csv\n",
    "print(\"\\nMenggabungkan dengan label sentiment...\")\n",
    "df_labeled = pd.read_csv('dataset_labeled.csv')\n",
    "df_final = df_final.merge(\n",
    "    df_labeled[['Teks_Komentar', 'sentiment']], \n",
    "    on='Teks_Komentar', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Sentiment berhasil digabungkan!\")\n",
    "print(f\"\\nDistribusi sentiment:\")\n",
    "print(df_final['sentiment'].value_counts())\n",
    "\n",
    "# Simpan hasil akhir\n",
    "output_file = 'data_preprocessed.csv'\n",
    "df_final[['Teks_Komentar', 'teks_final', 'sentiment']].to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"\\nüíæ Data berhasil disimpan ke '{output_file}'\")\n",
    "\n",
    "# Tampilkan contoh hasil\n",
    "print(f\"\\nüìã Contoh hasil akhir:\")\n",
    "for i in range(min(3, len(df_final))):\n",
    "    print(f\"\\n   [{i+1}] SENTIMENT: {df_final['sentiment'].iloc[i]}\")\n",
    "    print(f\"       ASLI     : {df_final['Teks_Komentar'].iloc[i][:60]}...\")\n",
    "    print(f\"       FINAL    : {df_final['teks_final'].iloc[i][:60]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ PREPROCESSING SELESAI!\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nüìå PIPELINE YANG DITERAPKAN:\")\n",
    "print(\"   ‚úÖ Tahap 1: Filter Konteks & Noise (Strategi LONGGAR)\")\n",
    "print(\"   ‚úÖ Tahap 2: Cleaning (URL, HTML, Hapus Emoji, Slang, Elongasi)\")\n",
    "print(\"   ‚úÖ Tahap 3: Tokenisasi + Negation Normalization (ga/gak ‚Üí tidak)\")\n",
    "print(\"   ‚úÖ Tahap 4: Stemming (skip keyword & 'tidak')\")\n",
    "print(\"   ‚úÖ Tahap 5: Stopword Removal (preserve negasi & keyword)\")\n",
    "print(\"   ‚úÖ Tahap 6: Gabung Kembali + Merge Sentiment Label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f7aabee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DETAIL HASIL SETIAP TAHAP PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "CONTOH 1\n",
      "================================================================================\n",
      "\n",
      "0. ASLI:\n",
      "   Sempat mikir mau pindah ke negara sebelah, ngeapply citizenship. Tapi yah, aku cinta negara dan hara...\n",
      "\n",
      "1. CLEANING (Tahap 2):\n",
      "   sempat mikir mau pindah ke negara sebelah ngeapply citizenship tapi yah aku cinta negara dan harapan...\n",
      "\n",
      "2. TOKENISASI (Tahap 3a):\n",
      "   ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'sebelah', 'ngeapply', 'citizenship', 'tapi', 'yah']...\n",
      "   (Total: 21 kata)\n",
      "\n",
      "3. NEGATION NORMALIZATION (Tahap 3b):\n",
      "   ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'sebelah', 'ngeapply', 'citizenship', 'tapi', 'yah']...\n",
      "   (Total: 21 kata)\n",
      "\n",
      "4. STEMMING (Tahap 4):\n",
      "   ['sempat', 'mikir', 'mau', 'pindah', 'negara', 'belah', 'ngeapply', 'citizenship', 'tapi', 'yah']...\n",
      "   (Total: 21 kata)\n",
      "\n",
      "5. STOPWORD REMOVAL (Tahap 5):\n",
      "   ['mikir', 'pindah', 'negara', 'belah', 'ngeapply', 'citizenship', 'yah', 'cinta', 'negara', 'harap']...\n",
      "   (Total: 14 kata)\n",
      "\n",
      "6. FINAL - GABUNG KEMBALI (Tahap 6):\n",
      "   mikir pindah negara belah ngeapply citizenship yah cinta negara harap moga negara sembuh cepat...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CONTOH 2\n",
      "================================================================================\n",
      "\n",
      "0. ASLI:\n",
      "   Kalo kabur mau kemana ke Singapur ,emang di Singapur tinggal dimana rakyat Singapur aja nggak punya ...\n",
      "\n",
      "1. CLEANING (Tahap 2):\n",
      "   kalau kabur mau kemana ke singapur emang di singapur tinggal dimana rakyat singapur saja tidak punya...\n",
      "\n",
      "2. TOKENISASI (Tahap 3a):\n",
      "   ['kalau', 'kabur', 'mau', 'kemana', 'singapur', 'emang', 'singapur', 'tinggal', 'dimana', 'rakyat']...\n",
      "   (Total: 22 kata)\n",
      "\n",
      "3. NEGATION NORMALIZATION (Tahap 3b):\n",
      "   ['kalau', 'kabur', 'mau', 'kemana', 'singapur', 'emang', 'singapur', 'tinggal', 'dimana', 'rakyat']...\n",
      "   (Total: 22 kata)\n",
      "\n",
      "4. STEMMING (Tahap 4):\n",
      "   ['kalau', 'kabur', 'mau', 'mana', 'singapur', 'emang', 'singapur', 'tinggal', 'mana', 'rakyat']...\n",
      "   (Total: 22 kata)\n",
      "\n",
      "5. STOPWORD REMOVAL (Tahap 5):\n",
      "   ['kabur', 'singapur', 'emang', 'singapur', 'tinggal', 'rakyat', 'singapur', 'tidak', 'rumah', 'nyicil']...\n",
      "   (Total: 13 kata)\n",
      "\n",
      "6. FINAL - GABUNG KEMBALI (Tahap 6):\n",
      "   kabur singapur emang singapur tinggal rakyat singapur tidak rumah nyicil negara kaya singapur...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CONTOH 3\n",
      "================================================================================\n",
      "\n",
      "0. ASLI:\n",
      "   Klo sudah gelap susah terangnya lebih baik bubar...\n",
      "\n",
      "1. CLEANING (Tahap 2):\n",
      "   kalau sudah gelap susah terangnya lebih baik bubar...\n",
      "\n",
      "2. TOKENISASI (Tahap 3a):\n",
      "   ['kalau', 'sudah', 'gelap', 'susah', 'terangnya', 'lebih', 'baik', 'bubar']...\n",
      "   (Total: 8 kata)\n",
      "\n",
      "3. NEGATION NORMALIZATION (Tahap 3b):\n",
      "   ['kalau', 'sudah', 'gelap', 'susah', 'terangnya', 'lebih', 'baik', 'bubar']...\n",
      "   (Total: 8 kata)\n",
      "\n",
      "4. STEMMING (Tahap 4):\n",
      "   ['kalau', 'sudah', 'gelap', 'susah', 'terang', 'lebih', 'baik', 'bubar']...\n",
      "   (Total: 8 kata)\n",
      "\n",
      "5. STOPWORD REMOVAL (Tahap 5):\n",
      "   ['gelap', 'susah', 'terang', 'bubar']...\n",
      "   (Total: 4 kata)\n",
      "\n",
      "6. FINAL - GABUNG KEMBALI (Tahap 6):\n",
      "   gelap susah terang bubar...\n",
      "\n",
      "================================================================================\n",
      "\n",
      "*** PERHATIKAN ***\n",
      "   - Kata negasi (ga/gak/enggak) ‚Üí dinormalisasi ke 'tidak'\n",
      "   - Format: 'gaji tidak naik' (2 token terpisah, bukan TIDAK_naik)\n",
      "   - TF-IDF bigram akan menangkap: 'tidak naik', 'tidak bagus', dll\n",
      "   - Emoji telah DIHAPUS dari data\n",
      "   - Keyword penting (pajak, gaji, dll) tidak di-stem\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lihat detail preprocessing untuk 3 contoh pertama\n",
    "print(\"=\"*80)\n",
    "print(\"DETAIL HASIL SETIAP TAHAP PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(3, len(df_final))):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CONTOH {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n0. ASLI:\")\n",
    "    print(f\"   {df_final['Teks_Komentar'].iloc[i][:100]}...\")\n",
    "    \n",
    "    print(f\"\\n1. CLEANING (Tahap 2):\")\n",
    "    print(f\"   {df_final['teks_tahap1'].iloc[i][:100]}...\")\n",
    "    \n",
    "    print(f\"\\n2. TOKENISASI (Tahap 3a):\")\n",
    "    tokens_2 = df_final['tokens_tahap2'].iloc[i]\n",
    "    print(f\"   {tokens_2[:10]}...\")\n",
    "    print(f\"   (Total: {len(tokens_2)} kata)\")\n",
    "    \n",
    "    print(f\"\\n3. NEGATION NORMALIZATION (Tahap 3b):\")\n",
    "    tokens_2_5 = df_final['tokens_tahap2_5'].iloc[i]\n",
    "    print(f\"   {tokens_2_5[:10]}...\")\n",
    "    print(f\"   (Total: {len(tokens_2_5)} kata)\")\n",
    "    \n",
    "    print(f\"\\n4. STEMMING (Tahap 4):\")\n",
    "    tokens_3 = df_final['tokens_tahap3'].iloc[i]\n",
    "    print(f\"   {tokens_3[:10]}...\")\n",
    "    print(f\"   (Total: {len(tokens_3)} kata)\")\n",
    "    \n",
    "    print(f\"\\n5. STOPWORD REMOVAL (Tahap 5):\")\n",
    "    tokens_4 = df_final['tokens_tahap4'].iloc[i]\n",
    "    print(f\"   {tokens_4[:10]}...\")\n",
    "    print(f\"   (Total: {len(tokens_4)} kata)\")\n",
    "    \n",
    "    print(f\"\\n6. FINAL - GABUNG KEMBALI (Tahap 6):\")\n",
    "    print(f\"   {df_final['teks_final'].iloc[i][:100]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"\\n*** PERHATIKAN ***\")\n",
    "print(\"   - Kata negasi (ga/gak/enggak) ‚Üí dinormalisasi ke 'tidak'\")\n",
    "print(\"   - Format: 'gaji tidak naik' (2 token terpisah, bukan TIDAK_naik)\")\n",
    "print(\"   - TF-IDF bigram akan menangkap: 'tidak naik', 'tidak bagus', dll\")\n",
    "print(\"   - Emoji telah DIHAPUS dari data\")\n",
    "print(\"   - Keyword penting (pajak, gaji, dll) tidak di-stem\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
