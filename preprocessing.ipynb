{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24c43890",
   "metadata": {},
   "source": [
    "# Preprocessing Data Komentar YouTube\n",
    "\n",
    "Notebook ini melakukan preprocessing data komentar dengan langkah-langkah:\n",
    "1. Filter Konteks (Strategi KETAT)\n",
    "2. Bersihkan Teks (URL, Unicode, Emoji, lowercase, tanda baca, angka)\n",
    "3. Tokenization\n",
    "4. Hapus Stopwords\n",
    "5. Gabungkan Kembali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19494fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rhzain\\AppData\\Local\\Temp\\ipykernel_29372\\1846252205.py:8: FutureWarning: The demoji.download_codes attribute is deprecated and will be removed from demoji in a future version. It is an unused attribute as emoji codes are now distributed directly with the demoji package.\n",
      "  demoji.download_codes()\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import demoji\n",
    "\n",
    "# Download emoji data (run once)\n",
    "demoji.download_codes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2385c25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total komentar: 2196\n",
      "\n",
      "Contoh data awal:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Video_ID",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Teks_Komentar",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "a1504a89-110a-4b12-b613-a4240c34274e",
       "rows": [
        [
         "0",
         "MIo4tGN11j0",
         "berdoa tidak ada comment judol, amin."
        ],
        [
         "1",
         "MIo4tGN11j0",
         "kereen."
        ],
        [
         "2",
         "MIo4tGN11j0",
         "Sempat mikir mau pindah ke negara sebelah, ngeapply citizenship. Tapi yah, aku cinta negara dan harapanku semoga negara ini bisa sembuh secepatnya."
        ],
        [
         "3",
         "MIo4tGN11j0",
         "Kalo kabur mau kemana ke Singapur ,emang di Singapur tinggal dimana rakyat Singapur aja nggak punya rumah mereka masih nyicilüòÇ\"katanya negara kaya Singapur \""
        ],
        [
         "4",
         "MIo4tGN11j0",
         "Jg ke malaysia.....pergi sono india bangla nepal serumpun....rupiah rupie"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Video_ID</th>\n",
       "      <th>Teks_Komentar</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>berdoa tidak ada comment judol, amin.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>kereen.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>Sempat mikir mau pindah ke negara sebelah, nge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>Kalo kabur mau kemana ke Singapur ,emang di Si...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>MIo4tGN11j0</td>\n",
       "      <td>Jg ke malaysia.....pergi sono india bangla nep...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Video_ID                                      Teks_Komentar\n",
       "0  MIo4tGN11j0              berdoa tidak ada comment judol, amin.\n",
       "1  MIo4tGN11j0                                            kereen.\n",
       "2  MIo4tGN11j0  Sempat mikir mau pindah ke negara sebelah, nge...\n",
       "3  MIo4tGN11j0  Kalo kabur mau kemana ke Singapur ,emang di Si...\n",
       "4  MIo4tGN11j0  Jg ke malaysia.....pergi sono india bangla nep..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "df = pd.read_csv('raw-scrape-yt.csv')\n",
    "print(f\"Total komentar: {len(df)}\")\n",
    "print(\"\\nContoh data awal:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8fc2edc",
   "metadata": {},
   "source": [
    "## 1. Filter Konteks\n",
    "\n",
    "Membuang komentar noise yang tidak relevan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7732cbcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total keyword unik untuk filter konteks: 36\n",
      "Komentar sebelum filter: 2196\n",
      "Komentar setelah filter: 641\n",
      "Komentar terbuang: 1555\n"
     ]
    }
   ],
   "source": [
    "# --- FILTER 1: FILTER NOISE (Kode Anda, sudah bagus) ---\n",
    "def filter_noise(teks):\n",
    "    \"\"\"\n",
    "    Return True jika komentar VALID (BUKAN noise).\n",
    "    Return False jika komentar adalah NOISE (spam, pendek, dll).\n",
    "    \"\"\"\n",
    "    if pd.isna(teks) or len(teks.strip()) < 3:\n",
    "        return False # Ini adalah noise\n",
    "    \n",
    "    teks_lower = teks.lower().strip()\n",
    "    \n",
    "    # Daftar pola noise yang harus dibuang\n",
    "    noise_patterns = [\n",
    "        r'^[@#]',  # Dimulai dengan @ atau #\n",
    "        r'^\\d+$',  # Hanya angka\n",
    "        r'^[^\\w\\s]+$',  # Hanya simbol/emoji\n",
    "        r'(subscribe|subs|subrek|gw klik subs)',  # Promosi subscribe\n",
    "        r'(like|liek|liak|laik)\\s*(dulu|dong|yuk)',  # Ajakan like\n",
    "        r'(pin|ping|pin dong|pinn)',  # Minta pin\n",
    "        r'^(wkwk|haha|xixi|kwkw|wkwkwk|hahaha)+$',  # Hanya ketawa\n",
    "        r'^(amin|amiin|aamiin)+$',  # Hanya amin\n",
    "        r'^(pertamax|first|kedua|ketiga)',  # Klaim urutan\n",
    "        r'(giveaway|give away|kontes)',  # Promo\n",
    "        r'^(yang \\d{4})',  # \"yang 2024\", \"yang 2025\" (biasanya noise)\n",
    "        r'(judol|judi online)', # Filter judi online\n",
    "    ]\n",
    "    \n",
    "    for pattern in noise_patterns:\n",
    "        if re.search(pattern, teks_lower):\n",
    "            return False # Ini adalah noise\n",
    "    \n",
    "    # Komentar terlalu pendek (< 10 karakter)\n",
    "    if len(teks_lower) < 10:\n",
    "        return False # Ini adalah noise\n",
    "    \n",
    "    return True # Jika lolos semua = BUKAN noise\n",
    "\n",
    "# --- FILTER 2: FILTER KONTEKS (Versi LONGGAR yang Efisien) ---\n",
    "KEYWORD = [\n",
    "    'kabur', 'pindah negara', 'leave indo', 'pajak', 'gaji', 'umr', \n",
    "    'biaya hidup', 'korupsi', 'sandwich', 'luar negeri', \n",
    "    'paspor', 'warga negara', 'singapur', 'singapore', 'australia', 'aussie', 'jepang', 'eropa', 'wni'\n",
    "    'capek', 'lelah', 'pemerintah', 'mending', 'percuma', 'beban', \n",
    "    'suram', 'males', 'ga jelas', 'ga ada harapan', 'nyicil', 'susah',\n",
    "    'setuju', 'sulit', 'stres', 'politik', 'birokrasi', 'konoha'\n",
    "]\n",
    "\n",
    "print(f\"Total keyword unik untuk filter konteks: {len(KEYWORD)}\")\n",
    "\n",
    "def filter_konteks(teks):\n",
    "    \"\"\"\n",
    "    Return True jika komentar RELEVAN (Strategi LONGGAR dan Efisien).\n",
    "    Return False jika di luar konteks.\n",
    "    \"\"\"\n",
    "    teks_lower = str(teks).lower()\n",
    "    \n",
    "    # Cukup cek satu kali terhadap list gabungan\n",
    "    if any(kata in teks_lower for kata in KEYWORD):\n",
    "        return True # Lolos jika mengandung SALAH SATU\n",
    "        \n",
    "    return False\n",
    "\n",
    "# --- FUNGSI GABUNGAN (YANG HARUS DITERAPKAN) ---\n",
    "def filter_final(teks):\n",
    "    # 1. Cek apakah ini NOISE?\n",
    "    if not filter_noise(teks):\n",
    "        return False # Jika \"wkwkwk\", buang.\n",
    "    \n",
    "    # 2. Jika bukan noise, cek KONTEKS?\n",
    "    if not filter_konteks(teks): # <- Memanggil fungsi baru yang efisien\n",
    "        return False # Jika \"resep nasi goreng\", buang.\n",
    "        \n",
    "    # Hanya lolos jika BUKAN noise DAN RELEVAN\n",
    "    return True\n",
    "\n",
    "# --- Terapkan Filter ---\n",
    "# Ganti 'TSeks_Komentar' dengan nama kolom Anda yang berisi teks mentah\n",
    "df['is_valid'] = df['Teks_Komentar'].apply(filter_final)\n",
    "\n",
    "df_filtered = df[df['is_valid']].copy()\n",
    "df_filtered = df_filtered.drop('is_valid', axis=1)\n",
    "\n",
    "print(f\"Komentar sebelum filter: {len(df)}\")\n",
    "print(f\"Komentar setelah filter: {len(df_filtered)}\")\n",
    "print(f\"Komentar terbuang: {len(df) - len(df_filtered)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e655cd7",
   "metadata": {},
   "source": [
    "## 2. Preprocessing Teks\n",
    "\n",
    "Melakukan preprocessing data dengan 5 tahap terpisah yang modular"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7e080c03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "INISIALISASI PREPROCESSING TOOLS\n",
      "============================================================\n",
      "‚úÖ Sastrawi Stemmer berhasil dimuat\n",
      "‚úÖ Stopwords berhasil dimuat:\n",
      "   - Indonesia: 757 kata\n",
      "   - Inggris  : 198 kata\n",
      "   - Gaul     : 40 kata\n",
      "   - TOTAL    : 992 kata\n",
      "\n",
      "============================================================\n",
      "‚úÖ Semua fungsi preprocessing sudah siap!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "import demoji\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# --- 1. Inisialisasi Alat (Sekali saja) ---\n",
    "print(\"=\"*60)\n",
    "print(\"INISIALISASI PREPROCESSING TOOLS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Impor Stemmer dari Sastrawi\n",
    "try:\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    print(\"‚úÖ Sastrawi Stemmer berhasil dimuat\")\n",
    "    factory = StemmerFactory()\n",
    "    stemmer_sastrawi = factory.create_stemmer()\n",
    "except ImportError:\n",
    "    print(\"‚ùå ERROR: Library Sastrawi tidak ditemukan.\")\n",
    "    print(\"   Silakan instal dengan: pip install Sastrawi\")\n",
    "    raise\n",
    "\n",
    "# --- 2. Buat Daftar Stopwords Gabungan ---\n",
    "stop_words_indo = set(stopwords.words('indonesian'))\n",
    "stop_words_eng = set(stopwords.words('english'))\n",
    "custom_stopwords_gaul = {\n",
    "    'yg', 'dg', 'rt', 'dgn', 'ny', 'd', 'klo', 'kalo', 'amp', 'biar', 'bkn', 'na', \n",
    "    'nya', 'nih', 'sih', 'si', 'tau', 'tuh', 'utk', 'ya', 'ga', 'gak', 'gaes',\n",
    "    'bang', 'bro', 'sob', 'gw', 'gua', 'lu', 'lo', 'wkwk', 'haha', 'wkwkwk', \n",
    "    'amin', 'amiin', 'aamiin', 'yuk', 'dong', 'deh', 'kok', 'sih'\n",
    "}\n",
    "\n",
    "# Gabungkan semua stopwords\n",
    "stop_words_final = stop_words_indo.union(stop_words_eng, custom_stopwords_gaul)\n",
    "\n",
    "print(f\"‚úÖ Stopwords berhasil dimuat:\")\n",
    "print(f\"   - Indonesia: {len(stop_words_indo)} kata\")\n",
    "print(f\"   - Inggris  : {len(stop_words_eng)} kata\")\n",
    "print(f\"   - Gaul     : {len(custom_stopwords_gaul)} kata\")\n",
    "print(f\"   - TOTAL    : {len(stop_words_final)} kata\")\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "\n",
    "\n",
    "# --- 3. Definisi Fungsi Preprocessing ---\n",
    "\n",
    "def tahap1_cleaning(teks):\n",
    "    \"\"\"\n",
    "    Tahap 1: Membersihkan teks mentah dari URL, HTML, emoji, \n",
    "    tanda baca, dan angka.\n",
    "    \"\"\"\n",
    "    if pd.isna(teks):\n",
    "        return \"\"\n",
    "    \n",
    "    teks = str(teks)\n",
    "    \n",
    "    # Hapus URL\n",
    "    teks = re.sub(r'http\\S+|www\\.\\S+', '', teks)\n",
    "    \n",
    "    # Hapus HTML tags\n",
    "    teks = re.sub(r'<.*?>', '', teks)\n",
    "    \n",
    "    # Ganti Emoji ke deskripsi\n",
    "    teks = demoji.replace_with_desc(teks, sep=\" \")\n",
    "    \n",
    "    # Normalisasi Unicode (font aneh)\n",
    "    teks = unicodedata.normalize('NFKD', teks)\n",
    "    \n",
    "    # Lowercase\n",
    "    teks = teks.lower()\n",
    "    \n",
    "    # Normalisasi kata berulang (capeeeek -> capek)\n",
    "    teks = re.sub(r'(.)\\1{2,}', r'\\1', teks)\n",
    "    \n",
    "    # Hapus tanda baca dan angka (hanya sisakan huruf dan spasi)\n",
    "    teks = re.sub(r'[^a-z\\s]', ' ', teks)\n",
    "    \n",
    "    # Hapus spasi berlebih\n",
    "    teks = re.sub(r'\\s+', ' ', teks).strip()\n",
    "    \n",
    "    return teks\n",
    "\n",
    "\n",
    "def tahap2_tokenisasi(teks):\n",
    "    \"\"\"\n",
    "    Tahap 2: Tokenisasi - Memecah teks menjadi list kata-kata.\n",
    "    \"\"\"\n",
    "    if not teks or pd.isna(teks):\n",
    "        return []\n",
    "    \n",
    "    # Pecah berdasarkan spasi\n",
    "    tokens = teks.split()\n",
    "    \n",
    "    # Filter kata yang terlalu pendek (< 3 karakter)\n",
    "    tokens = [kata for kata in tokens if len(kata) >= 3]\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "\n",
    "def tahap3_stopword_removal(tokens):\n",
    "    \"\"\"\n",
    "    Tahap 3: Hapus Stopwords dari list tokens.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    # Hapus stopwords\n",
    "    tokens_bersih = [kata for kata in tokens if kata not in stop_words_final]\n",
    "    \n",
    "    return tokens_bersih\n",
    "\n",
    "\n",
    "def tahap4_stemming(tokens):\n",
    "    \"\"\"\n",
    "    Tahap 4: Stemming - Mengubah kata ke bentuk dasar.\n",
    "    (Ini proses LAMBAT, dilakukan per-kata)\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return []\n",
    "    \n",
    "    # Stem setiap kata\n",
    "    tokens_stemmed = [stemmer_sastrawi.stem(kata) for kata in tokens]\n",
    "    \n",
    "    return tokens_stemmed\n",
    "\n",
    "\n",
    "def tahap5_gabung_kembali(tokens):\n",
    "    \"\"\"\n",
    "    Tahap 5: Gabungkan tokens menjadi teks final.\n",
    "    \"\"\"\n",
    "    if not tokens:\n",
    "        return \"\"\n",
    "    \n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"‚úÖ Semua fungsi preprocessing sudah siap!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6beeb1d1",
   "metadata": {},
   "source": [
    "### Tahap 1: Cleaning\n",
    "\n",
    "Membersihkan teks dari URL, HTML, emoji, tanda baca, dan angka"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "66bba9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 1/5: CLEANING\n",
      "============================================================\n",
      "Membersihkan URL, HTML, emoji, tanda baca, dan angka...\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil cleaning:\n",
      "ASLI  : Kalo kabur mau kemana ke Singapur ,emang di Singapur tinggal dimana ra...\n",
      "CLEAN : kalo kabur mau kemana ke singapur emang di singapur tinggal dimana rak...\n",
      "============================================================\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil cleaning:\n",
      "ASLI  : Kalo kabur mau kemana ke Singapur ,emang di Singapur tinggal dimana ra...\n",
      "CLEAN : kalo kabur mau kemana ke singapur emang di singapur tinggal dimana rak...\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 1/5: CLEANING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Membersihkan URL, HTML, emoji, tanda baca, dan angka...\")\n",
    "\n",
    "df_filtered['teks_tahap1'] = df_filtered['Teks_Komentar'].apply(tahap1_cleaning)\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"\\nContoh hasil cleaning:\")\n",
    "print(f\"ASLI  : {df_filtered['Teks_Komentar'].iloc[0][:70]}...\")\n",
    "print(f\"CLEAN : {df_filtered['teks_tahap1'].iloc[0][:70]}...\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4906f4a",
   "metadata": {},
   "source": [
    "### Tahap 2: Tokenisasi\n",
    "\n",
    "Memecah teks menjadi list kata-kata dan filter kata pendek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63d5e1b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 2/5: TOKENISASI\n",
      "============================================================\n",
      "Memecah teks menjadi kata-kata...\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil tokenisasi:\n",
      "TEKS   : kalo kabur mau kemana ke singapur emang di singapur tinggal ...\n",
      "TOKENS : ['kalo', 'kabur', 'mau', 'kemana', 'singapur', 'emang', 'singapur', 'tinggal']...\n",
      "JUMLAH : 26 kata\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 2/5: TOKENISASI\")\n",
    "print(\"=\"*60)\n",
    "print(\"Memecah teks menjadi kata-kata...\")\n",
    "\n",
    "df_filtered['tokens_tahap2'] = df_filtered['teks_tahap1'].apply(tahap2_tokenisasi)\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"\\nContoh hasil tokenisasi:\")\n",
    "tokens_contoh = df_filtered['tokens_tahap2'].iloc[0]\n",
    "print(f\"TEKS   : {df_filtered['teks_tahap1'].iloc[0][:60]}...\")\n",
    "print(f\"TOKENS : {tokens_contoh[:8]}...\")\n",
    "print(f\"JUMLAH : {len(tokens_contoh)} kata\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c84efc2",
   "metadata": {},
   "source": [
    "### Tahap 3: Stopword Removal\n",
    "\n",
    "Menghapus stopwords (Indonesia, Inggris, dan kata gaul)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "553bb734",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 3/5: STOPWORD REMOVAL\n",
      "============================================================\n",
      "Menghapus stopwords...\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil stopword removal:\n",
      "SEBELUM : ['kalo', 'kabur', 'mau', 'kemana', 'singapur', 'emang', 'singapur', 'tinggal']...\n",
      "SESUDAH : ['kabur', 'kemana', 'singapur', 'emang', 'singapur', 'tinggal', 'dimana', 'rakyat']...\n",
      "JUMLAH  : 26 kata ‚Üí 19 kata\n",
      "REDUKSI : 7 kata dihapus\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 3/5: STOPWORD REMOVAL\")\n",
    "print(\"=\"*60)\n",
    "print(\"Menghapus stopwords...\")\n",
    "\n",
    "df_filtered['tokens_tahap3'] = df_filtered['tokens_tahap2'].apply(tahap3_stopword_removal)\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"\\nContoh hasil stopword removal:\")\n",
    "tokens_before = df_filtered['tokens_tahap2'].iloc[0]\n",
    "tokens_after = df_filtered['tokens_tahap3'].iloc[0]\n",
    "print(f\"SEBELUM : {tokens_before[:8]}...\")\n",
    "print(f\"SESUDAH : {tokens_after[:8]}...\")\n",
    "print(f\"JUMLAH  : {len(tokens_before)} kata ‚Üí {len(tokens_after)} kata\")\n",
    "print(f\"REDUKSI : {len(tokens_before) - len(tokens_after)} kata dihapus\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c54c741",
   "metadata": {},
   "source": [
    "### Tahap 4: Stemming\n",
    "\n",
    "Mengubah kata ke bentuk dasar (proses lambat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f266a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 4/5: STEMMING\n",
      "============================================================\n",
      "Mengubah kata ke bentuk dasar...\n",
      "‚ö†Ô∏è  Proses ini lambat, harap sabar...\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil stemming:\n",
      "SEBELUM : ['kabur', 'kemana', 'singapur', 'emang', 'singapur']\n",
      "SESUDAH : ['kabur', 'mana', 'singapur', 'emang', 'singapur']\n",
      "============================================================\n",
      "‚úÖ Selesai!\n",
      "\n",
      "Contoh hasil stemming:\n",
      "SEBELUM : ['kabur', 'kemana', 'singapur', 'emang', 'singapur']\n",
      "SESUDAH : ['kabur', 'mana', 'singapur', 'emang', 'singapur']\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 4/5: STEMMING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Mengubah kata ke bentuk dasar...\")\n",
    "print(\"‚ö†Ô∏è  Proses ini lambat, harap sabar...\")\n",
    "\n",
    "df_filtered['tokens_tahap4'] = df_filtered['tokens_tahap3'].apply(tahap4_stemming)\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"\\nContoh hasil stemming:\")\n",
    "tokens_before = df_filtered['tokens_tahap3'].iloc[0]\n",
    "tokens_after = df_filtered['tokens_tahap4'].iloc[0]\n",
    "print(f\"SEBELUM : {tokens_before[:5]}\")\n",
    "print(f\"SESUDAH : {tokens_after[:5]}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "055e1b00",
   "metadata": {},
   "source": [
    "### Tahap 5: Gabung Kembali & Simpan\n",
    "\n",
    "Menggabungkan tokens menjadi teks final dan menyimpan hasil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7aabee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üìù TAHAP 5/5: GABUNG KEMBALI & SIMPAN\n",
      "============================================================\n",
      "Menggabungkan tokens menjadi teks final...\n",
      "‚úÖ Selesai!\n",
      "\n",
      "üìä Ringkasan:\n",
      "   Total komentar valid   : 641\n",
      "   Komentar kosong terbuang : 0\n",
      "\n",
      "üíæ Data berhasil disimpan ke 'data_preprocessed.csv'\n",
      "\n",
      "üìã Contoh hasil akhir:\n",
      "   ASLI  : Kalo kabur mau kemana ke Singapur ,emang di Singapur tinggal...\n",
      "   FINAL : kabur mana singapur emang singapur tinggal mana rakyat singa...\n",
      "\n",
      "============================================================\n",
      "üéâ PREPROCESSING SELESAI!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"üìù TAHAP 5/5: GABUNG KEMBALI & SIMPAN\")\n",
    "print(\"=\"*60)\n",
    "print(\"Menggabungkan tokens menjadi teks final...\")\n",
    "\n",
    "df_filtered['teks_final'] = df_filtered['tokens_tahap4'].apply(tahap5_gabung_kembali)\n",
    "\n",
    "# Buang baris dengan teks kosong setelah preprocessing\n",
    "df_final = df_filtered[df_filtered['teks_final'].str.strip() != ''].copy()\n",
    "\n",
    "print(\"‚úÖ Selesai!\")\n",
    "print(f\"\\nüìä Ringkasan:\")\n",
    "print(f\"   Total komentar valid   : {len(df_final):,}\")\n",
    "print(f\"   Komentar kosong terbuang : {len(df_filtered) - len(df_final):,}\")\n",
    "\n",
    "# Simpan hasil akhir\n",
    "output_file = 'data_preprocessed.csv'\n",
    "df_final[['Teks_Komentar', 'teks_final']].to_csv(output_file, index=False, encoding='utf-8')\n",
    "print(f\"\\nüíæ Data berhasil disimpan ke '{output_file}'\")\n",
    "\n",
    "# Tampilkan contoh hasil\n",
    "print(f\"\\nüìã Contoh hasil akhir:\")\n",
    "print(f\"   ASLI  : {df_final['Teks_Komentar'].iloc[0][:60]}...\")\n",
    "print(f\"   FINAL : {df_final['teks_final'].iloc[0][:60]}...\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéâ PREPROCESSING SELESAI!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367dd406",
   "metadata": {},
   "source": [
    "## 3. Lihat Detail Hasil Preprocessing (Opsional)\n",
    "\n",
    "Melihat hasil setiap tahap preprocessing untuk beberapa contoh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c32ada77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DETAIL HASIL SETIAP TAHAP PREPROCESSING\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "CONTOH 1\n",
      "================================================================================\n",
      "\n",
      "0. ASLI:\n",
      "   Kalo kabur mau kemana ke Singapur ,emang di Singapur tinggal dimana rakyat Singapur aja nggak punya ...\n",
      "\n",
      "1. CLEANING:\n",
      "   kalo kabur mau kemana ke singapur emang di singapur tinggal dimana rakyat singapur aja nggak punya r...\n",
      "\n",
      "2. TOKENISASI:\n",
      "   ['kalo', 'kabur', 'mau', 'kemana', 'singapur', 'emang', 'singapur', 'tinggal', 'dimana', 'rakyat']...\n",
      "   (Total: 26 kata)\n",
      "\n",
      "3. STOPWORD REMOVAL:\n",
      "   ['kabur', 'kemana', 'singapur', 'emang', 'singapur', 'tinggal', 'dimana', 'rakyat', 'singapur', 'aja']...\n",
      "   (Total: 19 kata)\n",
      "\n",
      "4. STEMMING:\n",
      "   ['kabur', 'mana', 'singapur', 'emang', 'singapur', 'tinggal', 'mana', 'rakyat', 'singapur', 'aja']...\n",
      "   (Total: 19 kata)\n",
      "\n",
      "5. FINAL (Gabungan):\n",
      "   kabur mana singapur emang singapur tinggal mana rakyat singapur aja nggak rumah nyicil face tears jo...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CONTOH 2\n",
      "================================================================================\n",
      "\n",
      "0. ASLI:\n",
      "   Klo sudah gelap susah terangnya lebih baik bubar...\n",
      "\n",
      "1. CLEANING:\n",
      "   klo sudah gelap susah terangnya lebih baik bubar...\n",
      "\n",
      "2. TOKENISASI:\n",
      "   ['klo', 'sudah', 'gelap', 'susah', 'terangnya', 'lebih', 'baik', 'bubar']...\n",
      "   (Total: 8 kata)\n",
      "\n",
      "3. STOPWORD REMOVAL:\n",
      "   ['gelap', 'susah', 'terangnya', 'bubar']...\n",
      "   (Total: 4 kata)\n",
      "\n",
      "4. STEMMING:\n",
      "   ['gelap', 'susah', 'terang', 'bubar']...\n",
      "   (Total: 4 kata)\n",
      "\n",
      "5. FINAL (Gabungan):\n",
      "   gelap susah terang bubar...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "CONTOH 3\n",
      "================================================================================\n",
      "\n",
      "0. ASLI:\n",
      "   Pengen kabur tapi gak punya uangüò≠...\n",
      "\n",
      "1. CLEANING:\n",
      "   pengen kabur tapi gak punya uang loudly crying face...\n",
      "\n",
      "2. TOKENISASI:\n",
      "   ['pengen', 'kabur', 'tapi', 'gak', 'punya', 'uang', 'loudly', 'crying', 'face']...\n",
      "   (Total: 9 kata)\n",
      "\n",
      "3. STOPWORD REMOVAL:\n",
      "   ['pengen', 'kabur', 'uang', 'loudly', 'crying', 'face']...\n",
      "   (Total: 6 kata)\n",
      "\n",
      "4. STEMMING:\n",
      "   ['ken', 'kabur', 'uang', 'loudly', 'crying', 'face']...\n",
      "   (Total: 6 kata)\n",
      "\n",
      "5. FINAL (Gabungan):\n",
      "   ken kabur uang loudly crying face...\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lihat detail preprocessing untuk 3 contoh pertama\n",
    "print(\"=\"*80)\n",
    "print(\"DETAIL HASIL SETIAP TAHAP PREPROCESSING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i in range(min(3, len(df_final))):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"CONTOH {i+1}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n0. ASLI:\")\n",
    "    print(f\"   {df_final['Teks_Komentar'].iloc[i][:100]}...\")\n",
    "    \n",
    "    print(f\"\\n1. CLEANING:\")\n",
    "    print(f\"   {df_final['teks_tahap1'].iloc[i][:100]}...\")\n",
    "    \n",
    "    print(f\"\\n2. TOKENISASI:\")\n",
    "    tokens_2 = df_final['tokens_tahap2'].iloc[i]\n",
    "    print(f\"   {tokens_2[:10]}...\")  # Tampilkan 10 token pertama\n",
    "    print(f\"   (Total: {len(tokens_2)} kata)\")\n",
    "    \n",
    "    print(f\"\\n3. STOPWORD REMOVAL:\")\n",
    "    tokens_3 = df_final['tokens_tahap3'].iloc[i]\n",
    "    print(f\"   {tokens_3[:10]}...\")  # Tampilkan 10 token pertama\n",
    "    print(f\"   (Total: {len(tokens_3)} kata)\")\n",
    "    \n",
    "    print(f\"\\n4. STEMMING:\")\n",
    "    tokens_4 = df_final['tokens_tahap4'].iloc[i]\n",
    "    print(f\"   {tokens_4[:10]}...\")  # Tampilkan 10 token pertama\n",
    "    print(f\"   (Total: {len(tokens_4)} kata)\")\n",
    "    \n",
    "    print(f\"\\n5. FINAL (Gabungan):\")\n",
    "    print(f\"   {df_final['teks_final'].iloc[i][:100]}...\")\n",
    "    print()\n",
    "\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeb966d",
   "metadata": {},
   "source": [
    "## 4. Statistik Preprocessing\n",
    "\n",
    "Menampilkan statistik lengkap dari proses preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "842125df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "üìä STATISTIK PREPROCESSING LENGKAP\n",
      "======================================================================\n",
      "\n",
      "üìà JUMLAH DATA:\n",
      "   Data awal                      :      2,196 komentar\n",
      "   Setelah filter konteks         :        641 komentar\n",
      "   Data akhir valid               :        641 komentar\n",
      "   Persentase data valid          :      29.19%\n",
      "   Data terbuang                  :      1,555 komentar\n",
      "\n",
      "üìù RATA-RATA PANJANG TEKS (karakter):\n",
      "   Teks asli                      :     209.97\n",
      "   Teks final                     :     118.22\n",
      "   Reduksi                        :      43.70%\n",
      "\n",
      "üìÑ RATA-RATA JUMLAH KATA:\n",
      "   Teks asli                      :      32.66 kata\n",
      "   Teks final                     :      18.47 kata\n",
      "   Reduksi                        :      43.43%\n",
      "\n",
      "üîÑ RATA-RATA TOKENS PER TAHAP:\n",
      "   Setelah Tokenisasi             :      31.22 tokens\n",
      "   Setelah Stopword Removal       :      18.47 tokens\n",
      "   Setelah Stemming               :      18.47 tokens\n",
      "   Reduksi total tokens           :      40.83%\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Preprocessing berhasil!\n",
      "üíæ File tersimpan: data_preprocessed.csv\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Hitung statistik lengkap\n",
    "total_awal = len(df)\n",
    "total_setelah_filter = len(df_filtered)\n",
    "total_final = len(df_final)\n",
    "persentase_valid = (total_final / total_awal * 100) if total_awal > 0 else 0\n",
    "\n",
    "# Hitung rata-rata panjang teks\n",
    "avg_len_asli = df_final['Teks_Komentar'].str.len().mean()\n",
    "avg_len_final = df_final['teks_final'].str.len().mean()\n",
    "\n",
    "# Hitung rata-rata jumlah kata\n",
    "avg_words_asli = df_final['Teks_Komentar'].str.split().str.len().mean()\n",
    "avg_words_final = df_final['teks_final'].str.split().str.len().mean()\n",
    "\n",
    "# Hitung statistik tokens\n",
    "avg_tokens_setelah_tokenisasi = df_final['tokens_tahap2'].apply(len).mean()\n",
    "avg_tokens_setelah_stopword = df_final['tokens_tahap3'].apply(len).mean()\n",
    "avg_tokens_setelah_stemming = df_final['tokens_tahap4'].apply(len).mean()\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"üìä STATISTIK PREPROCESSING LENGKAP\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìà JUMLAH DATA:\")\n",
    "print(f\"   {'Data awal':<30} : {total_awal:>10,} komentar\")\n",
    "print(f\"   {'Setelah filter konteks':<30} : {total_setelah_filter:>10,} komentar\")\n",
    "print(f\"   {'Data akhir valid':<30} : {total_final:>10,} komentar\")\n",
    "print(f\"   {'Persentase data valid':<30} : {persentase_valid:>10.2f}%\")\n",
    "print(f\"   {'Data terbuang':<30} : {total_awal - total_final:>10,} komentar\")\n",
    "\n",
    "print(f\"\\nüìù RATA-RATA PANJANG TEKS (karakter):\")\n",
    "print(f\"   {'Teks asli':<30} : {avg_len_asli:>10.2f}\")\n",
    "print(f\"   {'Teks final':<30} : {avg_len_final:>10.2f}\")\n",
    "print(f\"   {'Reduksi':<30} : {((avg_len_asli - avg_len_final) / avg_len_asli * 100):>10.2f}%\")\n",
    "\n",
    "print(f\"\\nüìÑ RATA-RATA JUMLAH KATA:\")\n",
    "print(f\"   {'Teks asli':<30} : {avg_words_asli:>10.2f} kata\")\n",
    "print(f\"   {'Teks final':<30} : {avg_words_final:>10.2f} kata\")\n",
    "print(f\"   {'Reduksi':<30} : {((avg_words_asli - avg_words_final) / avg_words_asli * 100):>10.2f}%\")\n",
    "\n",
    "print(f\"\\nüîÑ RATA-RATA TOKENS PER TAHAP:\")\n",
    "print(f\"   {'Setelah Tokenisasi':<30} : {avg_tokens_setelah_tokenisasi:>10.2f} tokens\")\n",
    "print(f\"   {'Setelah Stopword Removal':<30} : {avg_tokens_setelah_stopword:>10.2f} tokens\")\n",
    "print(f\"   {'Setelah Stemming':<30} : {avg_tokens_setelah_stemming:>10.2f} tokens\")\n",
    "print(f\"   {'Reduksi total tokens':<30} : {((avg_tokens_setelah_tokenisasi - avg_tokens_setelah_stemming) / avg_tokens_setelah_tokenisasi * 100):>10.2f}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"‚úÖ Preprocessing berhasil!\")\n",
    "print(f\"üíæ File tersimpan: {output_file}\")\n",
    "print(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
