{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8545127",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core libraries loaded successfully\n",
      "indoNLP not found - using fallback mode\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import unicodedata\n",
    "from typing import List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "try:\n",
    "    import demoji\n",
    "    from nltk.corpus import stopwords\n",
    "    from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "    print(\"Core libraries loaded successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    print(\"Install required packages: pip install demoji nltk Sastrawi\")\n",
    "    raise\n",
    "\n",
    "try:\n",
    "    from indonlp.preprocessing import replace_slang, replace_word_elongation\n",
    "    INDONLP_AVAILABLE = True\n",
    "    print(\"indoNLP loaded\")\n",
    "except ImportError:\n",
    "    INDONLP_AVAILABLE = False\n",
    "    replace_slang = lambda x: x\n",
    "    replace_word_elongation = lambda x: x\n",
    "    print(\"indoNLP not found - using fallback mode\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e38e644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded (Cleaned)\n"
     ]
    }
   ],
   "source": [
    "class Config:\n",
    "    INPUT_FILE = 'data_filtered.csv'\n",
    "    LABEL_FILE = 'dataset_labeled.csv'\n",
    "    OUTPUT_FILE = 'data_preprocessed.csv'\n",
    "    \n",
    "    KEYWORDS = [\n",
    "        'pemerintah', 'penduduk', 'pengangguran', 'pendapatan', \n",
    "        'perusahaan', 'pemimpin', 'mematikan', 'kemiskinan', \n",
    "        'keadilan', 'kebijakan', 'berantakan',\n",
    "    ]\n",
    "    \n",
    "    NEGATION_WORDS = {\n",
    "        'tidak', 'bukan', 'jangan', 'ga', 'gak', 'enggak', \n",
    "        'nggak', 'ndak', 'engga', 'belum', 'tanpa'\n",
    "    }\n",
    "    \n",
    "    CUSTOM_STOPWORDS = {\n",
    "        'yg', 'dg', 'rt', 'dgn', 'ny', 'd', 'klo', 'kalo', 'amp', \n",
    "        'biar', 'bkn', 'na', 'nya', 'nih', 'sih', 'si', 'tau', 'tuh', \n",
    "        'utk', 'ya', 'gaes', 'bang', 'bro', 'sob', 'gw', 'gua', 'lu', \n",
    "        'lo', 'wkwk', 'haha', 'wkwkwk', 'amin', 'amiin', 'aamiin', \n",
    "        'yuk', 'dong', 'deh', 'kok', \n",
    "        'government', 'tax', 'salary', 'system', 'netizen', \n",
    "        'the', 'and', 'or', 'in', 'of', 'to', 'is', 'for', 'it', 'on', 'at'\n",
    "    }\n",
    "\n",
    "print(\"Configuration loaded (Cleaned)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7c42708",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PreprocessingTools:\n",
    "    def __init__(self, config: Config):\n",
    "        self.config = config\n",
    "        self._initialize_tools()\n",
    "        self._build_keyword_set()\n",
    "        self._build_stopwords()\n",
    "        \n",
    "    def _initialize_tools(self):\n",
    "        factory = StemmerFactory()\n",
    "        self.stemmer = factory.create_stemmer()\n",
    "        \n",
    "    def _build_keyword_set(self):\n",
    "        self.keyword_set = set()\n",
    "        for keyword in self.config.KEYWORDS:\n",
    "            self.keyword_set.update(keyword.lower().split())\n",
    "        \n",
    "    def _build_stopwords(self):\n",
    "        stop_indo = set(stopwords.words('indonesian'))\n",
    "        \n",
    "        self.stopwords = (\n",
    "            stop_indo.union(self.config.CUSTOM_STOPWORDS)\n",
    "            - self.config.NEGATION_WORDS \n",
    "            - self.keyword_set\n",
    "        )\n",
    "\n",
    "config = Config()\n",
    "tools = PreprocessingTools(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ff0b5a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing pipeline initialized (Optimized: Cleaning -> Negation -> Stopwords -> Stemming)\n"
     ]
    }
   ],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, tools: PreprocessingTools):\n",
    "        self.tools = tools\n",
    "        \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        text = str(text)\n",
    "        text = re.sub(r'http\\S+|www\\.\\S+', '', text)\n",
    "        text = re.sub(r'<.*?>', '', text)\n",
    "        text = demoji.replace(text, '')\n",
    "        text = unicodedata.normalize('NFKD', text)\n",
    "        text = text.lower()\n",
    "        \n",
    "        if INDONLP_AVAILABLE:\n",
    "            text = replace_slang(text)\n",
    "            text = replace_word_elongation(text)\n",
    "        \n",
    "        text = re.sub(r'[^a-z\\s]', ' ', text)\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def tokenize(self, text: str) -> List[str]:\n",
    "        if not text:\n",
    "            return []\n",
    "        return text.split()\n",
    "    \n",
    "    def normalize_negation(self, tokens: List[str]) -> List[str]:\n",
    "        normalized = []\n",
    "        for token in tokens:\n",
    "            if token in self.tools.config.NEGATION_WORDS:\n",
    "                normalized.append('tidak')\n",
    "            else:\n",
    "                normalized.append(token)\n",
    "        return normalized\n",
    "\n",
    "    def remove_stopwords(self, tokens: List[str]) -> List[str]:\n",
    "        filtered = []\n",
    "        for token in tokens:\n",
    "            if token == 'tidak':\n",
    "                filtered.append(token)\n",
    "                continue\n",
    "            \n",
    "            if token in self.tools.keyword_set:\n",
    "                filtered.append(token)\n",
    "                continue\n",
    "                \n",
    "            if token not in self.tools.stopwords:\n",
    "                filtered.append(token)\n",
    "                \n",
    "        return filtered\n",
    "\n",
    "    def stem_tokens(self, tokens: List[str]) -> List[str]:\n",
    "        stemmed = []\n",
    "        for token in tokens:\n",
    "            if token == 'tidak' or token in self.tools.keyword_set:\n",
    "                stemmed.append(token)\n",
    "            else:\n",
    "                stemmed.append(self.tools.stemmer.stem(token))\n",
    "        \n",
    "        return stemmed\n",
    "    \n",
    "    def process(self, text: str) -> Dict[str, any]:\n",
    "        result = {'original': text}\n",
    "        result['cleaned'] = self.clean_text(text)\n",
    "        result['tokens_raw'] = self.tokenize(result['cleaned'])\n",
    "        result['tokens_negation'] = self.normalize_negation(result['tokens_raw'])\n",
    "        result['tokens_filtered'] = self.remove_stopwords(result['tokens_negation'])\n",
    "        result['tokens_stemmed'] = self.stem_tokens(result['tokens_filtered'])\n",
    "        result['tokens_final'] = result['tokens_stemmed']\n",
    "        result['final_text'] = \" \".join(result['tokens_final'])\n",
    "        \n",
    "        return result\n",
    "\n",
    "preprocessor = TextPreprocessor(tools)\n",
    "print(\"Preprocessing pipeline initialized (Optimized: Cleaning -> Negation -> Stopwords -> Stemming)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ebd0462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,072 comments from 'data_filtered.csv'\n",
      "Data shape: (1072, 3)\n",
      "Columns: ['Video_ID', 'Teks_Komentar', 'text_normalized_temp']\n"
     ]
    }
   ],
   "source": [
    "def load_data(filepath: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(filepath)\n",
    "        print(f\"Loaded {len(df):,} comments from '{filepath}'\")\n",
    "        return df\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File '{filepath}' not found\")\n",
    "        raise\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading data: {e}\")\n",
    "        raise\n",
    "\n",
    "df = load_data(config.INPUT_FILE)\n",
    "print(f\"Data shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "be03ca20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data...\n",
      "Progress: 1072/1072 (100.0%)\n",
      "Processing complete: 1,072 valid, 0 empty removed\n"
     ]
    }
   ],
   "source": [
    "def process_dataframe(df: pd.DataFrame, preprocessor: TextPreprocessor) -> pd.DataFrame:\n",
    "    print(\"\\nProcessing data...\")\n",
    "    \n",
    "    text_col = 'text_normalized_temp' if 'text_normalized_temp' in df.columns else 'Teks_Komentar'\n",
    "    \n",
    "    results = []\n",
    "    for idx, text in enumerate(df[text_col]):\n",
    "        if idx % 100 == 0:\n",
    "            print(f\"Progress: {idx}/{len(df)} ({idx/len(df)*100:.1f}%)\", end='\\r')\n",
    "        \n",
    "        result = preprocessor.process(text)\n",
    "        results.append(result)\n",
    "    \n",
    "    print(f\"Progress: {len(df)}/{len(df)} (100.0%)\")\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    df_processed['teks_cleaned'] = [r['cleaned'] for r in results]\n",
    "    df_processed['tokens_final'] = [r['tokens_final'] for r in results]\n",
    "    df_processed['teks_final'] = [r['final_text'] for r in results]\n",
    "    \n",
    "    initial_count = len(df_processed)\n",
    "    df_processed = df_processed[df_processed['teks_final'].str.strip() != ''].copy()\n",
    "    removed_count = initial_count - len(df_processed)\n",
    "    \n",
    "    print(f\"Processing complete: {len(df_processed):,} valid, {removed_count:,} empty removed\")\n",
    "    \n",
    "    return df_processed\n",
    "\n",
    "df_processed = process_dataframe(df, preprocessor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "273907b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1,072 labeled records\n",
      "After removing duplicates: 1,068 unique records\n",
      "Merge complete: 1,072 records\n",
      "\n",
      "Sentiment distribution:\n",
      "sentiment\n",
      "Positif    691\n",
      "Netral     202\n",
      "Negatif    179\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "def merge_sentiment_labels(df: pd.DataFrame, label_file: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        df_labels = pd.read_csv(label_file)\n",
    "        print(f\"Loaded {len(df_labels):,} labeled records\")\n",
    "        \n",
    "        df_labels = df_labels.drop_duplicates(subset=['Teks_Komentar'], keep='first')\n",
    "        print(f\"After removing duplicates: {len(df_labels):,} unique records\")\n",
    "        \n",
    "        df_merged = df.merge(\n",
    "            df_labels[['Teks_Komentar', 'sentiment']], \n",
    "            on='Teks_Komentar', \n",
    "            how='left'\n",
    "        )\n",
    "        \n",
    "        print(f\"Merge complete: {len(df_merged):,} records\")\n",
    "        print(\"\\nSentiment distribution:\")\n",
    "        print(df_merged['sentiment'].value_counts())\n",
    "        \n",
    "        return df_merged\n",
    "        \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Warning: Label file '{label_file}' not found\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        print(f\"Warning: Error merging labels: {e}\")\n",
    "        return df\n",
    "\n",
    "df_final = merge_sentiment_labels(df_processed, config.LABEL_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eac5a890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to 'data_preprocessed.csv' (1,072 rows)\n"
     ]
    }
   ],
   "source": [
    "def save_results(df: pd.DataFrame, output_file: str):\n",
    "    if 'sentiment' in df.columns:\n",
    "        output_cols = ['Teks_Komentar', 'teks_final', 'sentiment']\n",
    "    else:\n",
    "        output_cols = ['Teks_Komentar', 'teks_final']\n",
    "    \n",
    "    df[output_cols].to_csv(output_file, index=False, encoding='utf-8')\n",
    "    print(f\"Data saved to '{output_file}' ({len(df):,} rows)\")\n",
    "\n",
    "save_results(df_final, config.OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "42d564c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preprocessing Examples:\n",
      "============================================================\n",
      "\n",
      "Example 1\n",
      "------------------------------------------------------------\n",
      "Sentiment: Negatif\n",
      "\n",
      "Original:\n",
      "Sempat mikir mau pindah ke negara sebelah, ngeapply citizenship. Tapi yah, aku cinta negara dan hara...\n",
      "\n",
      "Cleaned:\n",
      "sempat mikir mau pindah ke negara sebelah ngeapply citizenship tapi yah aku cinta negara dan harapan...\n",
      "\n",
      "Final:\n",
      "mikir pindah negara belah ngeapply citizenship yah cinta negara harap moga negara sembuh cepat...\n",
      "\n",
      "Tokens (14): ['mikir', 'pindah', 'negara', 'belah', 'ngeapply', 'citizenship', 'yah', 'cinta', 'negara', 'harap']...\n",
      "\n",
      "Example 2\n",
      "------------------------------------------------------------\n",
      "Sentiment: Negatif\n",
      "\n",
      "Original:\n",
      "Kalo kabur mau kemana ke Singapur ,emang di Singapur tinggal dimana rakyat Singapur aja nggak punya ...\n",
      "\n",
      "Cleaned:\n",
      "kalau kabur mau kemana ke singapur emang di singapur tinggal dimana rakyat singapur saja tidak punya...\n",
      "\n",
      "Final:\n",
      "kabur mana singapur emang singapur tinggal mana rakyat singapur tidak rumah nyicil negara kaya singa...\n",
      "\n",
      "Tokens (15): ['kabur', 'mana', 'singapur', 'emang', 'singapur', 'tinggal', 'mana', 'rakyat', 'singapur', 'tidak']...\n",
      "\n",
      "Example 3\n",
      "------------------------------------------------------------\n",
      "Sentiment: Positif\n",
      "\n",
      "Original:\n",
      "Klo sudah gelap susah terangnya lebih baik bubar...\n",
      "\n",
      "Cleaned:\n",
      "kalau sudah gelap susah terangnya lebih baik bubar...\n",
      "\n",
      "Final:\n",
      "gelap susah terang bubar...\n",
      "\n",
      "Tokens (4): ['gelap', 'susah', 'terang', 'bubar']...\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING COMPLETE\n",
      "============================================================\n",
      "\n",
      "Pipeline stages:\n",
      "1. Text Cleaning\n",
      "2. Tokenization\n",
      "3. Negation Normalization\n",
      "4. Stemming (preserve keywords)\n",
      "5. Stopword Removal (preserve negations)\n",
      "6. Text Reconstruction\n",
      "\n",
      "Final dataset: 1,072 records\n",
      "Output file: 'data_preprocessed.csv'\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "def display_examples(df: pd.DataFrame, n_examples: int = 3):\n",
    "    print(\"\\nPreprocessing Examples:\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    for i in range(min(n_examples, len(df))):\n",
    "        print(f\"\\nExample {i+1}\")\n",
    "        print(\"-\"*60)\n",
    "        \n",
    "        if 'sentiment' in df.columns:\n",
    "            print(f\"Sentiment: {df['sentiment'].iloc[i]}\")\n",
    "        \n",
    "        print(f\"\\nOriginal:\\n{df['Teks_Komentar'].iloc[i][:100]}...\")\n",
    "        print(f\"\\nCleaned:\\n{df['teks_cleaned'].iloc[i][:100]}...\")\n",
    "        print(f\"\\nFinal:\\n{df['teks_final'].iloc[i][:100]}...\")\n",
    "        \n",
    "        if 'tokens_final' in df.columns:\n",
    "            tokens = df['tokens_final'].iloc[i]\n",
    "            print(f\"\\nTokens ({len(tokens)}): {tokens[:10]}...\")\n",
    "\n",
    "display_examples(df_final)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PREPROCESSING COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "print(\"\\nPipeline stages:\")\n",
    "print(\"1. Text Cleaning\")\n",
    "print(\"2. Tokenization\")\n",
    "print(\"3. Negation Normalization\")\n",
    "print(\"4. Stemming (preserve keywords)\")\n",
    "print(\"5. Stopword Removal (preserve negations)\")\n",
    "print(\"6. Text Reconstruction\")\n",
    "print(f\"\\nFinal dataset: {len(df_final):,} records\")\n",
    "print(f\"Output file: '{config.OUTPUT_FILE}'\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
